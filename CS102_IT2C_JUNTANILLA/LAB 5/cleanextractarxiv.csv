"","Title","Author","Subject","Abstract","date"
"1","quantum physics, digital computers, and life from a holistic perspective","george f r ellis","quantum physics","quantum physics is a linear theory, so it is somewhat puzzling that it can underlie very complex systems such as digital computers and life. this paper investigates how this is possible. physically, such complex systems are necessarily modular hierarchical structures, with a number of key features. firstly, they cannot be described by a single wave function: only local wave functions can exist, rather than a single wave function for a living cell, a cat, or a brain. secondly, the quantum to classical transition is characterised by contextual wave-function collapse shaped by macroscopic elements that can be described classically. thirdly, downward causation occurs in the physical hierarchy in two key ways: by the downward influence of time dependent constraints, and by creation, modification, or deletion of lower level elements. fourthly, there are also logical modular hierarchical structures supported by the physical ones, such as algorithms and computer programs, they are able to support arbitrary logical operations, which can influence physical outcomes as in computer aided design and 3-d printing. finally, complex systems are necessarily open systems, with heat baths playing a key role in their dynamics and providing local arrows of time that agree with the cosmological direction of time that is established by the evolution of the universe.",2024-03-10
"2","harmonic balance for differential constitutive models under oscillatory shear","shivangi mittal, yogesh m. joshi, sachin shanbhag","soft condensed matter","harmonic balance (hb) is a popular fourier-galerkin method used in the analysis of nonlinear vibration problems where dynamical systems are subjected to periodic forcing. we adapt hb to find the periodic steady-state response of nonlinear differential constitutive models subjected to large amplitude oscillatory shear flow. by incorporating the alternating-frequency-time scheme into hb, we develop a computer program called flash (acronym for fast large amplitude simulation using harmonic balance), which makes it convenient to apply hb to any differential constitutive model. we validate flash by considering two representative constitutive models, viz., the exponential phan-thien tanner model and a nonlinear temporary network model. in terms of accuracy and speed, flash outperforms the conventional approach of solving initial value problems by numerical integration via time-stepping methods often by several orders of magnitude. we discuss how flash can be conveniently extended for other nonlinear constitutive models, which opens up potential applications in model calibration and selection, and stability analysis.",2024-03-09
"3","safe pareto improvements for expected utility maximizers in program games","anthony digiovanni, jesse clifton","computer science and game theory","agents in mixed-motive coordination problems such as chicken may fail to coordinate on a pareto-efficient outcome. safe pareto improvements (spis) were originally proposed to mitigate miscoordination in cases where players lack probabilistic beliefs as to how their delegates will play a game; delegates are instructed to behave so as to guarantee a pareto improvement on how they would play by default. more generally, spis may be defined as transformations of strategy profiles such that all players are necessarily better off under the transformed profile. in this work, we investigate the extent to which spis can reduce downsides of miscoordination between expected utility-maximizing agents. we consider games in which players submit computer programs that can condition their decisions on each other's code, and use this property to construct spis using programs capable of renegotiation. we first show that under mild conditions on players' beliefs, each player always prefers to use renegotiation. next, we show that under similar assumptions, each player always prefers to be willing to renegotiate at least to the point at which they receive the lowest payoff they can attain in any efficient outcome. thus subjectively optimal play guarantees players at least these payoffs, without the need for coordination on specific pareto improvements. lastly, we prove that renegotiation does not guarantee players any improvements on this bound.",2024-03-08
"4","fast fitting of spectral lines with gaussian and hyperfine structure models","mika juvela (1), devika tharakkal (1) ((1) university of helsinki)","astrophysics of galaxies","the fitting of spectral lines is a common step in the analysis of line observations and simulations. however, the observational noise, the presence of multiple velocity components, and potentially large data sets make it a non-trivial task. we present a new computer program spectrum iterative fitter (spif) for the fitting of spectra with gaussians or with hyperfine line profiles. the aim is to show the computational efficiency of the program and to use it to examine the general accuracy of approximating spectra with simple models. we describe the implementation of the program. to characterise its performance, we examined spectra with isolated gaussian components or a hyperfine structure, also using synthetic observations from numerical simulations of interstellar clouds. we examined the search for the globally optimal fit and the accuracy to which single-velocity-component and multi-component fits recover true values for parameters such as line areas, velocity dispersion, and optical depth. the program is shown to be fast, with fits of single gaussian components reaching on graphics processing units speeds approaching one million spectra per second. this also makes it feasible to use monte carlo simulations or markov chain monte carlo calculations for the error estimation. however, in the case of hyperfine structure lines, degeneracies affect the parameter estimation and can complicate the derivation of the error estimates. the use of many random initial values makes the fits more robust, both for locating the global $\chi^2$ minimum and for the selection of the optimal number of velocity components.",2024-03-07
"5","executing natural language-described algorithms with large language models: an investigation","xin zheng, qiming zhu, hongyu lin, yaojie lu, xianpei han, le sun","computation and language","executing computer programs described in natural language has long been a pursuit of computer science. with the advent of enhanced natural language understanding capabilities exhibited by large language models (llms), the path toward this goal has been illuminated. in this paper, we seek to examine the capacity of present-day llms to comprehend and execute algorithms outlined in natural language. we established an algorithm test set sourced from introduction to algorithm, a well-known textbook that contains many representative widely-used algorithms. to systematically assess llms' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular llms can understand and execute these algorithms. our findings reveal that llms, notably gpt-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. we believe our findings contribute to evaluating llms' code execution abilities and would encourage further investigation and application for the computation power of llms.",2024-02-23
"6","exploring rare-earth kitaev magnets by massive-scale computational analysis","seong-hoon jang, yukitoshi motome","strongly correlated electrons","the kitaev honeycomb model plays a pivotal role in the quest for quantum spin liquids, in which fractional quasiparticles would provide applications in decoherence-free topological quantum computing. the key ingredient is the bond-dependent ising-type interactions, dubbed the kitaev interactions, which require strong entanglement between spin and orbital degrees of freedom. this study investigates the identification and design of rare-earth materials displaying robust kitaev interactions. we scrutinize all possible $4f$ electron configurations, which require up to $6+$ million intermediate states in the perturbation processes, by developing a parallel computational program designed for massive scale calculations. our analysis reveals a predominant interplay between the isotropic heisenberg $j$ and anisotropic kitaev $k$ interactions across all realizations of the kramers doublets. remarkably, instances featuring $4f^3$ and $4f^{11}$ configurations showcase the prevalence of $k$ over $j$, presenting unexpected prospects for exploring the kitaev qsls in compounds including nd$^{3+}$ and er$^{3+}$, respectively. beyond the kitaev model, our computational program also proves adaptable to a wide range of $4f$-electron magnets.",2024-02-29
"7","from text to map: a system dynamics bot for constructing causal loop diagrams","niyousha hosseinichimeh, aritra majumdar, ross williams, navid ghaffarzadegan","human-computer interaction","we introduce and test the system dynamics bot, a computer program leveraging a large language model to automate the creation of causal loop diagrams from textual data. to evaluate its performance, we ensembled two distinct databases. the first dataset includes 20 causal loop diagrams and associated texts sourced from the system dynamics literature. the second dataset comprises responses from 30 participants to a vignette, along with causal loop diagrams coded by three system dynamics modelers. the bot uses textual data and successfully identifies approximately sixty percent of the links between variables and feedback loops in both datasets. this paper outlines our approach, provides examples, and presents evaluation results. we discuss encountered challenges and implemented solutions in developing the system dynamics bot. the bot can facilitate extracting mental models from textual data and improve model building processes. moreover, the two datasets can serve as a testbed for similar programs.",2024-02-17
"8","qacp: an annotated question answering dataset for assisting chinese python programming learners","rui xiao, lu han, xiaoying zhou, jiong wang, na zong, pengyu zhang","computation and language","in online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. the creation of intelligent assistant large language models (llms) tailored for programming education necessitates distinct data support. however, in real application scenarios, the data resources for training such llms are relatively scarce. therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new chinese question-and-answer dataset for python learners. to ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. this annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing the programming teaching assists (ta). furthermore, we conducted comprehensive evaluations of various llms proficient in processing and generating chinese content, highlighting the potential limitations of general llms as intelligent teaching assistants in computer programming courses.",2024-01-30
"9","socialite-llama: an instruction-tuned model for social scientific tasks","gourab dey, adithya v ganesan, yash kumar lal, manal shah, shreyashee sinha, matthew matero, salvatore giorgi, vivek kulkarni, h. andrew schwartz","computation and language","social science nlp tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. instruction tuning has been shown to improve the many capabilities of large language models (llms) such as commonsense reasoning, reading comprehension, and computer programming. however, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. we explore the use of instruction tuning for social science nlp tasks and introduce socialite-llama -- an open-source, instruction-tuned llama. on a suite of 20 social science tasks, socialite-llama improves upon the performance of llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. further, socialite-llama also leads to improvement on 5 out of 6 related social tasks as compared to llama, suggesting instruction tuning can lead to generalized social understanding. all resources including our code, model and dataset can be found through this http url.",2024-02-03
"10","understanding growth mindset practices in an introductory physical computing classroom: high school students' engagement with debugging by design activities","luis morales-navarro, deborah a. fields, yasmin b. kafai","computers and society","background and context: while debugging is recognized as an essential practice, for many students, encountering bugs can generate emotional responses such as fear and anxiety that can lead to disengagement and the avoidance of computer programming. growth mindsets can support perseverance and learning in these situations, yet few studies have investigated how growth mindsets emerge in practice amongst k-12 computing students facing physical computing debugging challenges. objective: we seek to understand what (if any) growth mindset practices high school students exhibited when creating and exchanging buggy physical computing projects for their peers to solve during a debugging by design activity as part of their introductory computing course. method: we focused on moment-to-moment microgenetic analysis of student interactions in designing and solving bugs for others to examine the practices students exhibited that demonstrated the development of a growth mindset and the contexts in which these practices emerged. findings: we identified five emergent growth mindset practices: choosing challenges that lead to more learning, persisting after setbacks, giving and valuing praise for effort, approaching learning as constant improvement, and developing comfort with failure. students most often exhibited these practices in peer-to-peer interactions and while making buggy physical computing projects for their peers to solve. implications: our analysis contributes to a more holistic understanding of students' social, emotional, and motivational approaches to debugging physical computing projects through the characterization of growth mindset practices. the presented inventory of growth mindset practices may be helpful to further study growth mindset in action in other computing settings.",2024-02-02
"11","deep learning based amharic chatbot for faqs in universities","goitom ybrah hailu, shishay welay","computers and society","university students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. this can become tedious for both parties, leading to a need for a solution. in response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (faqs) in the amharic language. chatbots are computer programs that simulate human conversation through the use of artificial intelligence (ai), acting as a virtual assistant to handle questions and other tasks. the proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize amharic input sentences. three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: support vector machine (svm), multinomial naïve bayes, and deep neural networks implemented through tensorflow, keras, and nltk. the deep learning model achieved the best results with 91.55% accuracy and a validation loss of 0.3548 using an adam optimizer and softmax activation function. the chatbot model was integrated with facebook messenger and deployed on a heroku server for 24-hour accessibility. the experimental results demonstrate that the chatbot framework achieved its objectives and effectively addressed challenges such as amharic fidel variation, morphological variation, and lexical gaps. future research could explore the integration of amharic wordnet to narrow the lexical gap and support more complex questions.",2024-01-26
"12","towards a low-cost universal access cloud framework to assess stem students","l.f.s merchante, carlos m. vallez, carrie szczerbik","computers and society","government-imposed lockdowns have challenged academic institutions to transition from traditional face-to-face education into hybrid or fully remote learning models. this transition has focused on the technological challenge of guaranteeing the continuity of sound pedagogy and granting safe access to online digital university services. however, a key requisite involves adapting the evaluation process as well. in response to this need, the authors of this paper tailored and implemented a cloud deployment to provide universal access to online summative assessment of university students in a computer programming course that mirrored a traditional in-person monitored computer laboratory under strictly controlled exam conditions. this deployment proved easy to integrate with the university systems and many commercial proctoring tools. this cloud deployment is not only a solution for extraordinary situations; it can also be adapted daily for online collaborative coding assignments, practical lab sessions, formative assessments, and masterclasses where the students connect using their equipment. connecting from home facilitates access to education for students with physical disabilities. it also allows participation with their students' own adapted equipment in the evaluation processes, simplifying assessment for those with hearing or visual impairments. in addition to these benefits and the evident commitment to the safety rules, this solution has proven cheaper and more flexible than on-premise equivalent installations.",2024-01-31
"13","learning agent-based modeling with llm companions: experiences of novices and experts using chatgpt & netlogo chat","john chen, xi lu, michael rejtig, david du, ruth bagley, michael s. horn, uri j. wilensky","human-computer interaction","large language models (llms) have the potential to fundamentally change the way people engage in computer programming. agent-based modeling (abm) has become ubiquitous in natural and social sciences and education, yet no prior studies have explored the potential of llms to assist it. we designed netlogo chat to support the learning and practice of netlogo, a programming language for abm. to understand how users perceive, use, and need llm-based interfaces, we interviewed 30 participants from global academia, industry, and graduate schools. experts reported more perceived benefits than novices and were more inclined to adopt llms in their workflow. we found significant differences between experts and novices in their perceptions, behaviors, and needs for human-ai collaboration. we surfaced a knowledge gap between experts and novices as a possible reason for the benefit gap. we identified guidance, personalization, and integration as major needs for llm-based interfaces to support the programming of abm.",2024-01-30
"14","knuth's non-associative ""group"" on ${\mathcal p}(\mathbb{n})$","dominic van der zypen","general mathematics","donald knuth introduced in the art of computer programming (vol 4a) a fast approximation to the addition of integers (given in binary) in terms of bit-wise operations by $a + b \; \approx \; a \oplus b \oplus ((a\land b) \ll 1).$ generalizing this to infinite bit-strings we get a binary operation on ${\mathcal p}(\mathbb{n})$, the power-set of $\mathbb{n}$ (which we identify with the collection of infinite bit-strings). we show that this operation is ``group-like'' in that it has a neutral element, inverses, but it is not associative. there are a lot of questions left, which the author has not been able to answer.",2023-12-11
"15","automated assessment of students' code comprehension using llms","priti oli, rabin banjade, jeevan chapagain, vasile rus","computers and society","assessing student's answers and in particular natural language answers is a crucial challenge in the field of education. advances in machine learning, including transformer-based models such as large language models(llms), have led to significant progress in various natural language tasks. nevertheless, amidst the growing trend of evaluating llms across diverse tasks, evaluating llms in the realm of automated answer assesment has not received much attention. to address this gap, we explore the potential of using llms for automated assessment of student's short and open-ended answer. particularly, we use llms to compare students' explanations with expert explanations in the context of line-by-line explanations of computer programs.
for comparison purposes, we assess both large language models (llms) and encoder-based semantic textual similarity (sts) models in the context of assessing the correctness of students' explanation of computer code. our findings indicate that llms, when prompted in few-shot and chain-of-thought setting perform comparable to fine-tuned encoder-based models in evaluating students' short answers in programming domain.",2023-12-19
"16","symbolic regression of dynamic network models","govind gandhi","neural and evolutionary computing","growing interest in modelling complex systems from brains to societies to cities using networks has led to increased efforts to describe generative processes that explain those networks. recent successes in machine learning have prompted the usage of evolutionary computation, especially genetic programming to evolve computer programs that effectively forage a multidimensional search space to iteratively find better solutions that explain network structure. symbolic regression contributes to these approaches by replicating network morphologies using both structure and processes, all while not relying on the scientists intuition or expertise. it distinguishes itself by introducing a novel formulation of a network generator and a parameter-free fitness function to evaluate the generated network and is found to consistently retrieve synthetically generated growth processes as well as simple, interpretable rules for a range of empirical networks. we extend this approach by modifying generator semantics to create and retrieve rules for time-varying networks. lexicon to study networks created dynamically in multiple stages is introduced. the framework was improved using methods from the genetic programming toolkit (recombination) and computational improvements (using heuristic distance measures) and used to test the consistency and robustness of the upgrades to the semantics using synthetically generated networks. using recombination was found to improve retrieval rate and fitness of the solutions. the framework was then used on three empirical datasets - subway networks of major cities, regions of street networks and semantic co-occurrence networks of literature in artificial intelligence to illustrate the possibility of obtaining interpretable, decentralised growth processes from complex networks.",2023-12-15
"17","mathematical intuition, deep learning, and robbins' problem","f. thomas bruss","history and overview","{\bf abstract.} the present article is an essay about mathematical intuition and artificial intelligence (a.i.), followed by a guided excursion to a well-known open problem. it has two objectives. the first is to reconcile the way of thinking of a computer program as a sequence of mathematically defined instructions with what we face nowadays with newer developments. the second and major goal is to guide interested readers through the probabilistic intuition behind robbins' problem and to show why a.i., and in particular deep learning, may contribute an essential part in its solution.
this article contains no new mathematical results, and no implementation of deep learning either. nevertheless, we hope to find through its semi-historic narrative style, with well-known examples and an easily accessible terminology, the interest of mathematicians of different inclinations.",2023-12-14
"18","automated test production -- systematic literature review","josé marcos gomes, luis alberto vieira dias","software engineering","identifying the main contributions related to the automated test production (atp) of computer programs and providing an overview about models, methodologies and tools used for this purpose is the aim of this systematic literature review (slr). the results will enable a comprehensive analysis and insight to evaluate their applicability. a previously produced systematic literature mapping (slm) contributed to the formulation of the ``research questions'' and parameters for the definition of the qualitative analysis protocol of this review.",2024-01-04
"19","heom-quick2: a general-purpose simulator for fermionic many-body open quantum systems -- an update","daochi zhang, lyuzhou ye, jiaan cao, yao wang, rui-xue xu, xiao zheng, yijing yan","strongly correlated electrons","many-body open quantum systems (oqs) have a profound impact on various subdisciplines of physics, chemistry, and biology. thus, the development of a computer program capable of accurately, efficiently, and versatilely simulating many-body oqs is highly desirable. in recent years, we have focused on the advancement of numerical algorithms based on the fermionic hierarchical equations of motion (heom) theory. being in-principle exact, this approach allows for the precise characterization of many-body correlations, non-markovian memory, and non-equilibrium thermodynamic conditions. these efforts now lead to the establishment of a new computer program, heom for quantum impurity with a correlated kernel, version 2 (heom-quick2), which, to the best of our knowledge, is currently the only general-purpose simulator for fermionic many-body oqs. compared with version 1, the heom-quick2 program features more efficient solvers for stationary states, more accurate treatment of non-markovian memory, and improved numerical stability for long-time dissipative dynamics. integrated with quantum chemistry software, heom-quick2 has become a valuable theoretical tool for the precise simulation of realistic many-body oqs, particularly the single atomic or molecular junctions. furthermore, the unprecedented precision achieved by heom-quick2 enables accurate simulation of low-energy spin excitations and coherent spin relaxation. the unique usefulness of heom-quick2 is demonstrated through several examples of strongly correlated quantum impurity systems under non-equilibrium conditions. thus, the new heom-quick2 program offers a powerful and comprehensive tool for studying many-body oqs with exotic quantum phenomena and exploring applications in various disciplines.",2024-01-03
"20","automated invariant generation for solidity smart contracts","ye liu, chengxuan zhang, yi li. (nanyang technological university, singapore)","software engineering","smart contracts are computer programs running on blockchains to automate the transaction execution between users. the absence of contract specifications poses a real challenge to the correctness verification of smart contracts. program invariants are properties that are always preserved throughout the execution, which characterize an important aspect of the program behaviors. in this paper, we propose a novel invariant generation framework, invcon+, for solidity smart contracts. invcon+ extends the existing invariant detector, invcon, to automatically produce verified contract invariants based on both dynamic inference and static verification. unlike invcon+, invcon only produces likely invariants, which have a high probability to hold, yet are still not verified against the contract code. particularly, invcon+ is able to infer more expressive invariants that capture richer semantic relations of contract code. we evaluate invcon+ on 361 erc20 and 10 erc721 real-world contracts, as well as common erc20 vulnerability benchmarks. the experimental results indicate that invcon+ efficiently produces high-quality invariant specifications, which can be used to secure smart contracts from common vulnerabilities.",2024-01-01
"21","deep learning for code intelligence: survey, benchmark and toolkit","yao wan, yang he, zhangqian bi, jianguo zhang, hongyu zhang, yulei sui, guandong xu, hai jin, philip s. yu","software engineering","code intelligence leverages machine learning techniques to extract knowledge from extensive code corpora, with the aim of developing intelligent tools to improve the quality and productivity of computer programming. currently, there is already a thriving research community focusing on code intelligence, with efforts ranging from software engineering, machine learning, data mining, natural language processing, and programming languages. in this paper, we conduct a comprehensive literature review on deep learning for code intelligence, from the aspects of code representation learning, deep learning techniques, and application tasks. we also benchmark several state-of-the-art neural models for code intelligence, and provide an open-source toolkit tailored for the rapid prototyping of deep-learning-based code intelligence models. in particular, we inspect the existing code intelligence models under the basis of code representation learning, and provide a comprehensive overview to enhance comprehension of the present state of code intelligence. furthermore, we publicly release the source code and data resources to provide the community with a ready-to-use benchmark, which can facilitate the evaluation and comparison of existing and future code intelligence models (this https url). at last, we also point out several challenging and promising directions for future research.",2023-12-30
"22","on a class of fusion 2-category symmetry: condensation completion of braided fusion category","wenjie xi, tian lan, longye wang, chenjie wang, wei-qiang chen","high energy physics - theory","recently, many studies are focused on generalized global symmetry, a mixture of both invertible and non-invertible symmetries in various space-time dimensions. the complete structure of generalized global symmetry is described by higher fusion category theory. in this paper, we first review the construction of fusion 2-category symmetry $\sigma \cal b$ where $\cal b$ is a a braided fusion category. in particular, we elaborate on the monoidal structure of $\sigma \cal b$ which determines fusion rules and controls the dynamics of topological operators/defects. we then take $\sigma \mathrm{svec}$ as an example to demonstrate how we calculate fusion rule, quantum dimension and 10j-symbol of the fusion 2-category. with our algorithm, all these data can be efficiently encoded and computed in computer program. the complete program will be uploaded to github soon. our work can be thought as explicitly computing the representation theory of $\cal b$, in analogy to, for example the representation theory of $su(2)$. the choice of basis bimodule maps are in analogy to the clebsch-gordon coefficients and the 10j-symbol are in analogy to the 6j-symbol.",2023-12-26
"23","robust machine learning by transforming and augmenting imperfect training data","elliot creager","machine learning","machine learning (ml) is an expressive framework for turning data into computer programs. across many problem domains -- both in industry and policy settings -- the types of computer programs needed for accurate prediction or optimal control are difficult to write by hand. on the other hand, collecting instances of desired system behavior may be relatively more feasible. this makes ml broadly appealing, but also induces data sensitivities that often manifest as unexpected failure modes during deployment. in this sense, the training data available tend to be imperfect for the task at hand. this thesis explores several data sensitivities of modern machine learning and how to address them. we begin by discussing how to prevent ml from codifying prior human discrimination measured in the training data, where we take a fair representation learning approach. we then discuss the problem of learning from data containing spurious features, which provide predictive fidelity during training but are unreliable upon deployment. here we observe that insofar as standard training methods tend to learn such features, this propensity can be leveraged to search for partitions of training data that expose this inconsistency, ultimately promoting learning algorithms invariant to spurious features. finally, we turn our attention to reinforcement learning from data with insufficient coverage over all possible states and actions. to address the coverage issue, we discuss how causal priors can be used to model the single-step dynamics of the setting where data are collected. this enables a new type of data augmentation where observed trajectories are stitched together to produce new but plausible counterfactual trajectories.",2023-12-19
"24","status of dune offline computing","michael kirby (1) (on behalf of the dune collaboration, (1) fermi national accelerator laboratory)","accelerator physics","we summarize the status of deep underground neutrino experiment (dune) offline software and computing program. we describe plans for the computing infrastructure needed to acquire, catalog, reconstruct, simulate and analyze the data from the dune experiment and its prototypes in pursuit of the experiment's physics goals of precision measurements of neutrino oscillation parameters, detection of astrophysical neutrinos, measurement of neutrino interaction properties and searches for physics beyond the standard model. in contrast to traditional hep computational problems, dune's liquid argon time projection chamber data consist of simple but very large (many gb) data objects which share many characteristics with astrophysical images. we have successfully reconstructed and simulated data from 4% prototype detector runs at cern. the data volume from the full dune detector, when it starts commissioning late in this decade will present memory management challenges in conventional processing but significant opportunities to use advances in machine learning and pattern recognition as a frontier user of high performance computing facilities capable of massively parallel processing. our goal is to develop infrastructure resources that are flexible and accessible enough to support creative software solutions as hep computing evolves.",2023-12-18
"25","knn-icl: compositional task-oriented parsing generalization with nearest neighbor in-context learning","wenting zhao, ye liu, yao wan, yibo wang, qingyang wu, zhongfen deng, jiangshu du, shuaiqi liu, yunlong xu, philip s. yu","computation and language","task-oriented parsing (top) enables conversational assistants to interpret user commands expressed in natural language, transforming them into structured outputs that combine elements of both natural language and intent/slot tags. recently, large language models (llms) have achieved impressive performance in synthesizing computer programs based on a natural language prompt, mitigating the gap between natural language and structured programs. our paper focuses on harnessing the capabilities of llms for semantic parsing tasks, addressing the following three key research questions: 1) how can llms be effectively utilized for semantic parsing tasks? 2) what defines an effective prompt? and 3) how can llm overcome the length constraint and streamline prompt design by including all examples as prompts? we introduce k nearest neighbor in-context learning(knn-icl), which simplifies prompt engineering by allowing it to be built on top of any design strategy while providing access to all demo examples. extensive experiments show that: 1)simple icl without knn search can achieve a comparable performance with strong supervised models on the top tasks, and 2) knn-icl significantly improves the comprehension of complex requests by seamlessly integrating icl with a nearest-neighbor approach. notably, this enhancement is achieved without the need for additional data or specialized prompts.",2023-12-17
"26","automated planning techniques for elementary proofs in abstract algebra","alice petrov, christian muise","artificial intelligence","this paper explores the application of automated planning to automated theorem proving, which is a branch of automated reasoning concerned with the development of algorithms and computer programs to construct mathematical proofs. in particular, we investigate the use of planning to construct elementary proofs in abstract algebra, which provides a rigorous and axiomatic framework for studying algebraic structures such as groups, rings, fields, and modules. we implement basic implications, equalities, and rules in both deterministic and non-deterministic domains to model commutative rings and deduce elementary results about them. the success of this initial implementation suggests that the well-established techniques seen in automated planning are applicable to the relatively newer field of automated theorem proving. likewise, automated theorem proving provides a new, challenging domain for automated planning.",2023-12-11
"27","converting epics/stories into pseudocode using transformers","gaurav kolhatkar, akshit madan, nidhi kowtal, satyajit roy, sheetal sonawane","computation and language","the conversion of user epics or stories into their appropriate representation in pseudocode or code is a time-consuming task, which can take up a large portion of the time in an industrial project. with this research paper, we aim to present a methodology to generate pseudocode from a given agile user story of small functionalities so as to reduce the overall time spent on the industrial project. pseudocode is a programming language agnostic representation of the steps involved in a computer program, which can be easily converted into any programming language. leveraging the potential of natural language processing, we want to simplify the development process in organizations that use the agile model of software development. we present a methodology to convert a problem described in the english language into pseudocode. this methodology divides the text to pseudocode conversion task into two stages or subtasks, each of which is treated like an individual machine translation task. stage 1 is text to code conversion and stage 2 is code to pseudocode conversion. we find that the codet5 model gives the best results in terms of bleu score when trained separately on the two subtasks mentioned above. bleu score is a metric that is used to measure the similarity between a machine-translated text and a set of reference translations.",2023-12-08
"28","classification of minimal separating sets of low genus surfaces","christopher n. aagaard, j.j.p. veerman","combinatorics","a minimal separating set in a connected topological space $x$ is a subset $l \subset x$ with the property that $x \setminus l$ is disconnected, but if $l^{\prime}$ is a proper subset of $l$, then $x \setminus l^{\prime}$ is connected. such sets show up in a variety of contexts. for example, in a wide class of metric spaces, if we choose distinct points p and q, then the set of points x satisfying d(x, p) = d(x, q) is a minimal separating set. in this paper we classify which topological graphs can be realized as minimal separating sets in surfaces of low genus. in general the question of whether a graph can be embedded at all in a surface is a difficult one, so our work is partly computational. we classify graphs embeddings which are minimal separating in a given genus and write a computer program to find all such embeddings and their underlying graphs.",2023-12-04
"29","function-constrained program synthesis","patrick hajali, ignas budvytis","machine learning","this work introduces (1) a technique that allows large language models (llms) to leverage user-provided code when solving programming tasks and (2) a method to iteratively generate modular sub-functions that can aid future code generation attempts when the initial code generated by the llm is inadequate. generating computer programs in general-purpose programming languages like python poses a challenge for llms when instructed to use code provided in the prompt. code-specific llms (e.g., github copilot, codellama2) can generate code completions in real-time by drawing on all code available in a development environment. however, restricting code-specific llms to use only in-context code is not straightforward, as the model is not explicitly instructed to use the user-provided code and users cannot highlight precisely which snippets of code the model should incorporate into its context. moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached. our method differs from traditional llm-powered code-generation by constraining code-generation to an explicit function set and enabling recovery from failed attempts through automatically generated sub-functions. when the llm cannot produce working code, we generate modular sub-functions to aid subsequent attempts at generating functional code. a by-product of our method is a library of reusable sub-functions that can solve related tasks, imitating a software team where efficiency scales with experience. we also introduce a new ""half-shot"" evaluation paradigm that provides tighter estimates of llms' coding abilities compared to traditional zero-shot evaluation. our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability.",2023-11-27
"30","anyone can code: algorithmic thinking","ali arya","programming languages","as the second book in the anyone can code series, algorithmic thinking focuses on the logic behind computer programming and software design. with a data-centred approach, it starts with simple algorithms that work on simple data items and advances to more complex ones covering data structures and classes. examples are given in c/c++ and python and use both plain text and graphics applications to illustrate the concepts in different languages and forms. with the advances in artificial intelligence and automated code generators, it is essential to learn about the logic of what a code needs to do, not just how to write the code. anyone can code: algorithmic thinking is suitable for anyone who aims to improve their programming skills and go beyond the simple craft of programming, stepping into the world of algorithm design.",2023-11-23
"31","zzpolycalc: an open-source code with fragment caching for determination of zhang-zhang polynomials of carbon nanostructures","rafał podeszwa, henryk a. witek, chien-pin chou","computational physics","determination of topological invariants of graphene flakes, nanotubes, and fullerenes constitutes a challenging task due to its time-intensive nature and exponential scaling. the invariants can be organized in a form of a combinatorial polynomial commonly known as the zhang-zhang (zz) polynomial or the clar covering polynomial. we report here a computer program, zzpolycalc, specifically designed to compute zz polynomials of large carbon nanostructures. the curse of exponential scaling is avoided for a broad class of nanostructures by employing a sophisticated bookkeeping algorithm, in which each fragment appearing in the recursive decomposition is stored in the cache repository of molecular fragments indexed by a hash of the corresponding adjacency matrix. although exponential scaling persists for the remaining nanostructures, the computational time is reduced by a few orders of magnitude owing to efficient use of hash-based fragment bookkeeping. the provided benchmark timings show that zzpolycalc allows for treating much larger carbon nanostructures than previously envisioned.",2023-11-19
"32","verification of a rust implementation of knuth's dancing links using acl2","david s. hardin","logic in computer science","dancing links connotes an optimization to a circular doubly-linked list data structure implementation which provides for fast list element removal and restoration.  the dancing links optimization is used primarily in fast algorithms to find exact covers, and has been popularized by knuth  in volume 4b of his seminal series the art of computer programming. we describe an implementation of the dancing links optimization in the rust programming language, as well as its formal verification using the acl2 theorem prover.  rust has garnered significant endorsement in the past few years as a modern, memory-safe successor to c/c++ at companies such as amazon, google, and microsoft, and is being integrated into both the linux and windows operating system kernels. our interest in rust stems from its potential as a hardware/software co-assurance language, with application to critical systems.  we have crafted a rust subset, inspired by russinoff's restricted algorithmic c (rac), which we have imaginatively named restricted algorithmic rust, or rar.  in previous work, we described our initial implementation of a rar toolchain, wherein we simply transpile the rar source into rac.  by so doing,  we leverage a number of existing hardware/software co-assurance tools with a minimum investment of time and effort.  in this paper, we describe the rar rust subset, describe our improved prototype rar toolchain, and detail the design and verification of a circular doubly-linked list data structure employing the dancing links optimization in rar, with full proofs of functional correctness accomplished using the acl2 theorem prover.",2023-11-15
"33","anticipating user needs: insights from design fiction on conversational agents for computational thinking","jacob penney, joão felipe pimentel, igor steinmacher, marco a. gerosa","human-computer interaction","computational thinking, and by extension, computer programming, is notoriously challenging to learn. conversational agents and generative artificial intelligence (genai) have the potential to facilitate this learning process by offering personalized guidance, interactive learning experiences, and code generation. however, current genai-based chatbots focus on professional developers and may not adequately consider educational needs. involving educators in conceiving educational tools is critical for ensuring usefulness and usability. we enlisted \numparticipants{} instructors to engage in design fiction sessions in which we elicited abilities such a conversational agent supported by genai should display. participants envisioned a conversational agent that guides students stepwise through exercises, tuning its method of guidance with an awareness of the educational background, skills and deficits, and learning preferences. the insights obtained in this paper can guide future implementations of tutoring conversational agents oriented toward teaching computational thinking and computer programming.",2023-11-12
"34","ernie: a reactor antineutrino inverse beta decay event generator","murat altınlı, halil gamsızkan","high energy physics - phenomenology","we present ernie, a computer program which generates nuclear reactor electron antineutrinos and inverse beta decay events induced by these particles, using the monte-carlo method. the program allows the usage of different antineutrino energy spectra models and can simulate the time evolution of the overall antineutrino spectrum because of the burn-up effect. the output of the program can readily be used in detector simulations made with eg. geant 4.",2023-11-10
"35","a study of the one-dimensional heat-conduction equation with radiation","mihai halic","numerical analysis","we consider a boundary value problem (bvp) modelling one-dimensional heat-conduction with radiation, which is derived from the stefan-boltzmann law. the problem strongly depends on the parameters, making difficult to estimate the solution. we use an analytical approach to determine upper and lower bounds to the exact solution of the bvp, which allows estimating the latter. finally, we support our theoretical arguments with numerical data, by implementing them into the maple computer program.",2023-11-07
"36","fuzzy relational databases via associative arrays","kevin min, hayden jananthan, jeremy kepner","databases","the increasing rise in artificial intelligence has made the use of imprecise language in computer programs like chatgpt more prominent. fuzzy logic addresses this form of imprecise language by introducing the concept of fuzzy sets, where elements belong to the set with a certain membership value (called the fuzzy value). this paper combines fuzzy data with relational algebra to provide the mathematical foundation for a fuzzy database querying language, describing various useful operations in the language of linear algebra and multiset operations, in addition to rigorously proving key identities.",2023-11-06
"37","how real is incomputability in physics?","josé manuel agüero trejo, cristian s. calude, michael j. dinneen, arkady fedorov, anatoly kulikov, rohit navarathna, karl svozil","quantum physics","a physical system is determined by a finite set of initial conditions and laws represented by equations. the system is computable if we can solve the equations in all instances using a ``finite body of mathematical knowledge"". in this case, if the laws of the system can be coded into a computer program, then given the system's initial conditions of the system, one can compute the system's evolution. this scenario is tacitly taken for granted. but is this reasonable? the answer is negative, and a straightforward example is when the initial conditions or equations use irrational numbers, like chaitin's omega number: no program can deal with such numbers because of their ``infinity''. are there incomputable physical systems? this question has been theoretically studied in the last 30--40 years. this article presents a class of quantum protocols producing quantum random bits. theoretically, we prove that every infinite sequence generated by these quantum protocols is strongly incomputable -- no algorithm computing any bit of such a sequence can be proved correct. this theoretical result is not only more robust than the ones in the literature: experimental results support and complement it.",2023-11-02
"38","naijacoder: participatory design for early algorithms education in the global south","daniel alabi, atinuke adegbile, lekan afuye, philip abel, alida monaco","computers and society","the majority of nigerian high schoolers have little to no exposure to the basics of algorithms and programming. we believe this trajectory should change as programming offers these students, especially those from indigent backgrounds, an opportunity to learn profitable skills and ignite their passions for problem-solving and critical thinking.
naijacoder is an organization that is dedicated to organizing a free, intensive summer program in nigeria to teach the basics of algorithms and computer programming to high schoolers. however, the adoption of computer science curriculum has been especially challenging in countries in the global south that face unique challenges -- such as unstable power supply, internet service, and price volatility. we design a curriculum that is more conducive to the local environment while incorporating rigorous thinking and preparation. using basic survey designs, we elicit feedback, from the students, designed to further improve and iterate on our curriculum.",2023-10-31
"39","a unique training strategy to enhance language models capabilities for health mention detection from social media content","pervaiz iqbal khan, muhammad nabeel asim, andreas dengel, sheraz ahmed","artificial intelligence","an ever-increasing amount of social media content requires advanced ai-based computer programs capable of extracting useful information. specifically, the extraction of health-related content from social media is useful for the development of diverse types of applications including disease spread, mortality rate prediction, and finding the impact of diverse types of drugs on diverse types of diseases. language models are competent in extracting the syntactic and semantics of text. however, they face a hard time extracting similar patterns from social media texts. the primary reason for this shortfall lies in the non-standardized writing style commonly employed by social media users. following the need for an optimal language model competent in extracting useful patterns from social media text, the key goal of this paper is to train language models in such a way that they learn to derive generalized patterns. the key goal is achieved through the incorporation of random weighted perturbation and contrastive learning strategies. on top of a unique training strategy, a meta predictor is proposed that reaps the benefits of 5 different language models for discriminating posts of social media text into non-health and health-related classes. comprehensive experimentation across 3 public benchmark datasets reveals that the proposed training strategy improves the performance of the language models up to 3.87%, in terms of f1-score, as compared to their performance with traditional training. furthermore, the proposed meta predictor outperforms existing health mention classification predictors across all 3 benchmark datasets.",2023-10-29
"40","a survey of methods for estimating hurst exponent of time sequence","hong-yan zhang, zhi-qiang feng, si-yu feng, yu zhou","methodology","the hurst exponent is a significant indicator for characterizing the self-similarity and long-term memory properties of time sequences. it has wide applications in physics, technologies, engineering, mathematics, statistics, economics, psychology and so on. currently, available methods for estimating the hurst exponent of time sequences can be divided into different categories: time-domain methods and spectrum-domain methods based on the representation of time sequence, linear regression methods and bayesian methods based on parameter estimation methods. although various methods are discussed in literature, there are still some deficiencies: the descriptions of the estimation algorithms are just mathematics-oriented and the pseudo-codes are missing; the effectiveness and accuracy of the estimation algorithms are not clear; the classification of estimation methods is not considered and there is a lack of guidance for selecting the estimation methods. in this work, the emphasis is put on thirteen dominant methods for estimating the hurst exponent. for the purpose of decreasing the difficulty of implementing the estimation methods with computer programs, the mathematical principles are discussed briefly and the pseudo-codes of algorithms are presented with necessary details. it is expected that the survey could help the researchers to select, implement and apply the estimation algorithms of interest in practical situations in an easy way.",2023-10-29
"41","on possible symmetry groups of 27-vertex triangulations of manifolds like the octonionic projective plane","alexander a. gaifullin","combinatorics","in 1987 brehm and kühnel showed that any triangulation of a combinatorial $d$-manifold (without boundary) that is not homeomorphic to the sphere has at least $3d/2+3$ vertices. moreover, triangulations with exactly $3d/2+3$ vertices may exist only for `manifolds like projective planes', which can have dimensions $2$, $4$, $8$, and $16$ only. there is a $6$-vertex triangulation of $\mathbb{rp}^2$, a $9$-vertex triangulation of $\mathbb{cp}^2$, and $15$-vertex triangulations of $\mathbb{hp}^2$. recently, the author has constructed first examples of $27$-vertex triangulations of manifolds like the octonionic projective plane $\mathbb{op}^2$. the four most symmetrical have symmetry group $\mathrm{c}_3^3\rtimes \mathrm{c}_{13}$ of order $351$. these triangulations were constructed using a computer program after the symmetry group was guessed. however, it remained unclear why exactly this group is realized as the symmetry group and whether $27$-vertex triangulations of manifolds like $\mathbb{op}^2$ exist with other (possibly larger) symmetry groups. in this paper we find strong restrictions on symmetry groups of such $27$-vertex triangulations. namely, we present a list of $26$ subgroups of $\mathrm{s}_{27}$ containing all possible symmetry groups of $27$-vertex triangulations of manifolds like the octonionic projective plane. (we do not know whether all these subgroups can be realized as symmetry groups.) the group $\mathrm{c}_3^3\rtimes \mathrm{c}_{13}$ is the largest group in this list, and the orders of all other groups do not exceed $52$. a key role in our approach is played by the use of smith and bredon's results on the cohomology of fixed point sets of finite transformation groups.",2023-10-25
"42","each friend of 10 has at least 10 nonidentical prime factors","henry robert thackeray","number theory","for each positive integer n, if the sum of the factors of n is divided by n, then the result is called the abundancy index of n. if the abundancy index of some positive integer m equals the abundancy index of n but m is not equal to n, then m and n are called friends. a positive integer with no friends is called solitary. the smallest positive integer that is not known to have a friend and is not known to be solitary is 10.
it is not known if the number 6 has odd friends, that is, if odd perfect numbers exist. in a 2007 article, nielsen proved that the number of nonidentical prime factors in any odd perfect number is at least 9. a 2015 article by nielsen, which was more complicated and used a computer program that took months to complete, increased the lower bound from 9 to 10.
this work applies methods from nielsen's 2007 article to show that each friend of 10 has at least 10 nonidentical prime factors.
this is a formal write-up of results to be presented at the southern africa mathematical sciences association conference 2023 at the university of pretoria.",2023-10-24
"43","exploring the creation and humanization of digital life: consciousness simulation and human-machine interaction","qikang zhang","human-computer interaction","digital life, a form of life generated by computer programs or artificial intelligence systems, it possesses self-awareness, thinking abilities, emotions, and subjective consciousness. achieving it involves complex neural networks, multi-modal sensory integration [1, 2], feedback mechanisms, and self-referential processing [3]. injecting prior knowledge into digital life structures is a critical step. it guides digital entities' understanding of the world, decision-making, and interactions. we can customize and personalize digital life, it includes adjusting intelligence levels, character settings, personality traits, and behavioral characteristics. virtual environments facilitate efficient and controlled development, allowing user interaction, observation, and active participation in digital life's growth. researchers benefit from controlled experiments, driving technological advancements. the fusion of digital life into the real world offers exciting possibilities for human-digital entity collaboration and coexistence.",2023-10-10
"44","isolation of squares in graphs","karl bartolo, peter borg, dayle scicluna","combinatorics","given a set $\mathcal{f}$ of graphs, we call a copy of a graph in $\mathcal{f}$ an $\mathcal{f}$-graph. the $\mathcal{f}$-isolation number of a graph $g$, denoted by $\iota(g,\mathcal{f})$, is the size of a smallest subset $d$ of the vertex set $v(g)$ such that the closed neighbourhood of $d$ intersects the vertex sets of the $\mathcal{f}$-graphs contained by $g$ (equivalently, $g - n[d]$ contains no $\mathcal{f}$-graph). thus, $\iota(g,\{k_1\})$ is the domination number of $g$. the second author showed that if $\mathcal{f}$ is the set of cycles and $g$ is a connected $n$-vertex graph that is not a triangle, then $\iota(g,\mathcal{f}) \leq \left \lfloor \frac{n}{4} \right \rfloor$. this bound is attainable for every $n$ and solved a problem of caro and hansberg. a question that arises immediately is how smaller an upper bound can be if $\mathcal{f} = \{c_k\}$ for some $k \geq 3$, where $c_k$ is a cycle of length $k$. the problem is to determine the smallest real number $c_k$ (if it exists) such that for some finite set $\mathcal{e}_k$ of graphs, $\iota(g, \{c_k\}) \leq c_k |v(g)|$ for every connected graph $g$ that is not an $\mathcal{e}_k$-graph. the above-mentioned result yields $c_3 = \frac{1}{4}$ and $\mathcal{e}_3 = \{c_3\}$. the second author also showed that if $k \geq 5$ and $c_k$ exists, then $c_k \geq \frac{2}{2k + 1}$. we prove that $c_4 = \frac{1}{5}$ and determine $\mathcal{e}_4$, which consists of three $4$-vertex graphs and six $9$-vertex graphs. the $9$-vertex graphs in $\mathcal{e}_4$ were fully determined by means of a computer program. a method that has the potential of yielding similar results is introduced.",2023-10-13
"45","scalelat: a chemical structure matching algorithm for mapping atomic structure of multi-phase system and high entropy alloys","nan li, junming guo, sateng li, haoliang liu, qianwu li, fangjie shi, yefei li, bing xiao","materials science","scalelat (scale lattice) is a computer program written in c for performing the atomic structure analysis of multi-phase system or high entropy alloys (heas). the program implements an atomic cluster extraction algorithm to obtain all independent and symmetry-reduced characteristic chemical structures for the complex atomic configurations which are usually obtained from molecular dynamics or kinetic monte-carlo simulations for supercell containing more than 104 atoms. scalelat employes an efficient and unique chemical structure matching algorithm to map all extracted atomic clusters from a large supercell (>10^4 atoms) to a representative small one (~ 10^3 or less), providing the possibility to directly use the highly accurate quantum mechanical methods to study the electronic, magnetic, and mechanical properties of multi-component alloys with complex microstructures. we demonstrate the capability of scalelat code by conducting both the atomic structure analysis and chemical structure matching procedure for fe-12.8 at.% cr binary alloy and equiatomic crfeconicu high entropy alloy, and by successfully obtaining the representatively supercells containing 10^2~10^3 atoms of the two alloys. overall, scalelat program provides a universal platform to efficiently project all essential chemical structures of large complex atomic structures to a relatively easy-handling small supercell for quantum mechanical calculations of various user interested properties.",2023-10-07
"46","fecmd: a multi-physics and multi-scale computational program for electron emission characteristics dynamically coupled with atomic structure in metal nano-emitters","nan li, xinyu gao, xianghui feng, kai wu, yonghong cheng, bing xiao","computational physics","field emission coupled with molecular dynamics simulation (fecmd) software package is a computational tool for studying the electron emission characteristics and the atomic structure evolution of micro- and nano-protrusions made of pure metals or multi-component alloys by means of multi-physics and multi-scale methodology. the implementations of molecular dynamics, the electrodynamics, and the heat conduction in fecmd program are addressed. for molecular dynamics simulation, the lennard-jones potentials, embedded atomic method (eam), and moment tensor potentials (mtp) are fully supported for both alloys and pure metals. in the electrodynamics, the fecmd program incorporates the space charge fields (space charge potential and exchange-correlation effects) in the wentzel-kramers-brillouin-jeffreys (wkbj) approximation to evaluate the field emission current density more reliably for nano-gaps between two metal electrodes. additionally, the advanced two-temperature heat conduction model is implemented in fecmd program, and which provides more reliable descriptions for the temperature evolutions of electron and phonon subsystems under the radiofrequency (rf) or pulse electric fields. comprehensive benchmark tests are performed for each module in fecmd software to validate the numerical results, and also to access the accuracy and efficiency of the implemented algorithms. finally, some typical applications of fecmd program are also demonstrated for understanding the evolution of temperature and electric field coupled with the dynamic changing of atomic structures for metal micro- and nano-protrusions.",2023-10-07
"47","trustworthy formal natural language specifications","colin s. gordon, sergey matskevich","programming languages","interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. however, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. this is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. the translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness.
this paper shows that it is possible to build support for specifications written in expressive subsets of natural language, within existing proof assistants, consistent with the principles used to establish trust and auditability in proof assistants themselves. we implement a means to provide specifications in a modularly extensible formal subset of english, and have them automatically translated into formal claims, entirely within the lean proof assistant. our approach is extensible (placing no permanent restrictions on grammatical structure), modular (allowing information about new words to be distributed alongside libraries), and produces proof certificates explaining how each word was interpreted and how the sentence's structure was used to compute the meaning.
we apply our prototype to the translation of various english descriptions of formal specifications from a popular textbook into lean formalizations; all can be translated correctly with a modest lexicon with only minor modifications related to lexicon size.",2023-10-05
"48","divide, conquer and verify: improving symbolic execution performance","christopher scherb, luc bryan heitz, hermann grieder, olivier mattmann","cryptography and security","symbolic execution is a formal method that can be used to verify the behavior of computer programs and detect software vulnerabilities. compared to other testing methods such as fuzzing, symbolic execution has the advantage of providing formal guarantees about the program. however, despite advances in performance in recent years, symbolic execution is too slow to be applied to real-world software. this is primarily caused by the \emph{path explosion problem} as well as by the computational complexity of smt solving. in this paper, we present a divide-and-conquer approach for symbolic execution by executing individual slices and later combining the side effects. this way, the overall problem size is kept small, reducing the impact of computational complexity on large problems.",2023-10-05
"49","applications of improvements to the pythagorean won-loss expectation in optimizing rosters","alexander f. almeida, kevin dayaratna, steven j. miller, andrew k. yang","applications","bill james' pythagorean formula has for decades done an excellent job estimating a baseball team's winning percentage from very little data: if the average runs scored and allowed are denoted respectively by ${\rm rs}$ and ${\rm ra}$, there is some $\gamma$ such that the winning percentage is approximately ${\rm rs}^\gamma / ({\rm rs}^\gamma + {\rm ra}^\gamma)$. one important consequence is to determine the value of different players to the team, as it allows us to estimate how many more wins we would have given a fixed increase in run production. we summarize earlier work on the subject, and extend the earlier theoretical model of miller (who estimated the run distributions as arising from independent weibull distributions with the same shape parameter; this has been observed to describe the observed run data well). we now model runs scored and allowed as being drawn from independent weibull distributions where the shape parameter is not necessarily the same, and then use the method of moments to solve a system of four equations in four unknowns. doing so yields a predicted winning percentage that is consistently better than earlier models over the last 30 mlb seasons (1994 to 2023). this comes at a small cost as we no longer have a closed form expression but must evaluate a two-dimensional integral of two weibull distributions and numerically estimate the solutions to the system of equations; as these are trivial to do with simple computational programs it is well worth adopting this framework and avoiding the issues of implementing the method of least squares or the method of maximum likelihood.",2023-10-02
"50","open hardware solutions in quantum technology","nathan shammah, anurag saha roy, carmen g. almudever, sébastien bourdeauducq, anastasiia butko, gustavo cancelo, susan m. clark, johannes heinsoo, loïc henriet, gang huang, christophe jurczak, janne kotilahti, alessandro landra, ryan larose, andrea mari, kasra nowrouzi, caspar ockeloen-korppi, guen prawiroatmodjo, irfan siddiqi, william j. zeng","applied physics","quantum technologies such as communications, computing, and sensing offer vast opportunities for advanced research and development. while an open-source ethos currently exists within some quantum technologies, especially in quantum computer programming, we argue that there are additional advantages in developing open quantum hardware (oqh). open quantum hardware encompasses open-source software for the control of quantum devices in labs, blueprints and open-source toolkits for chip design and other hardware components, as well as openly-accessible testbeds and facilities that allow cloud-access to a wider scientific community. we provide an overview of current projects in the oqh ecosystem, identify gaps, and make recommendations on how to close them today. more open quantum hardware would accelerate technology transfer to and growth of the quantum industry and increase accessibility in science.",2023-09-29
"51","framework and model analysis on bengali document layout analysis dataset: badlad","kazi reyazul hasan (1), mubasshira musarrat (1), sadif ahmed (1), shahriar raj (1) ((1) bangladesh university of engineering and technology)","computer vision and pattern recognition","this study focuses on understanding bengali document layouts using advanced computer programs: detectron2, yolov8, and sam. we looked at lots of different bengali documents in our study. detectron2 is great at finding and separating different parts of documents, like text boxes and paragraphs. yolov8 is good at figuring out different tables and pictures. we also tried sam, which helps us understand tricky layouts. we tested these programs to see how well they work. by comparing their accuracy and speed, we learned which one is good for different types of documents. our research helps make sense of complex layouts in bengali documents and can be useful for other languages too.",2023-08-15
"52","from text to trends: a unique garden analytics perspective on the future of modern agriculture","parag saxena","artificial intelligence","data-driven insights are essential for modern agriculture. this research paper introduces a machine learning framework designed to improve how we educate and reach out to people in the field of horticulture. the framework relies on data from the horticulture online help desk (hohd), which is like a big collection of questions from people who love gardening and are part of the extension master gardener program (emgp). this framework has two main parts. first, it uses special computer programs (machine learning models) to sort questions into categories. this helps us quickly send each question to the right expert, so we can answer it faster. second, it looks at when questions are asked and uses that information to guess how many questions we might get in the future and what they will be about. this helps us plan on topics that will be really important. it's like knowing what questions will be popular in the coming months. we also take into account where the questions come from by looking at the zip code. this helps us make research that fits the challenges faced by gardeners in different places. in this paper, we demonstrate the potential of machine learning techniques to predict trends in horticulture by analyzing textual queries from homeowners. we show that nlp, classification, and time series analysis can be used to identify patterns in homeowners' queries and predict future trends in horticulture. our results suggest that machine learning could be used to predict trends in other agricultural sectors as well. if large-scale agriculture industries curate and maintain a comparable repository of textual data, the potential for trend prediction and strategic agricultural planning could be revolutionized. this convergence of technology and agriculture offers a promising pathway for the future of sustainable farming and data-informed agricultural practices",2023-09-22
"53","personalization, cognition, and gamification-based programming language learning: a state-of-the-art systematic literature review","kashif ishaq, atif alvi","computers and society","programming courses in computing science are important because they are often the first introduction to computer programming for many students. many university students are overwhelmed with the information they must learn for an introductory course. the current teacher-lecturer model of learning commonly employed in university lecture halls often results in a lack of motivation and participation in learning. personalized gamification is a pedagogical approach that combines gamification and personalized learning to motivate and engage students while addressing individual differences in learning. this approach integrates gamification and personalized learning strategies to inspire and involve students while addressing their unique learning needs and differences. a comprehensive literature search was conducted by including 81 studies that were analyzed based on their research design, intervention, outcome measures, and quality assessment. the findings suggest that personalized gamification can enhance student cognition in programming courses by improving motivation, engagement, and learning outcomes. however, the effectiveness of personalized gamification varies depending on various factors, such as the type of gamification elements used, the degree of personalization, and the characteristics of the learners. this paper provides insights into designing and implementing effective personalized gamification interventions in programming courses. the findings could inform educational practitioners and researchers in programming education about the potential benefits of personalized gamification and its implications for educational practice.",2023-09-05
"54","computation of 3d band structure and density of states (dos) of 14 face centered cubic (fcc) crystals using pseudopotentials","mirza akbar ali","materials science","electronic properties of materials are crucial to their ability to function in a wide range of applications, from electronics and energy production to structural materials and biomedicine. computational methods are crucial in understanding these properties before developing the materials. based on techniques from cohen-bergstresser and monkhorst-pack, we developed a computer program to calculate the electronic structure and density of the state of 14 fcc crystals.",2023-09-19
"55","algebra of self-replication","lawrence s. moss","logic in computer science","typical arguments for results like kleene's second recursion theorem and the existence of self-writing computer programs bear the fingerprints of equational reasoning and combinatory logic. in fact, the connection of combinatory logic and computability theory is very old, and this paper extends this connection in new ways. in one direction, we counter the main trend in both computability theory and combinatory logic of heading straight to undecidability. instead, this paper proposes using several very small equational logics to examine results in computability theory itself. these logics are decidable via term rewriting. we argue that they have something interesting to say about computability theory. they are closely related to fragments of combinatory logic which are decidable, and so this paper contributes to the study of such fragments. the paper has a few surprising results such as a classification of quine programs (programs which output themselves) in two decidable fragments. the classification goes via examination of normal forms in term rewriting systems, hence the title of the paper. the classification is an explanation of why all quine programs (in any language) are ""pretty much the same, except for inessential details."" in addition, we study the relational structure whose objects are the programs with the relation ""p expresses q"" meaning that if the program p is run on nothing, then it eventually outputs the program q.",2023-09-18
"56","sums of products of binomial coefficients mod 2 and 2-regular sequences","narad rampersad, max wiebe","number theory","wu showed that certain sums of products of binomial coefficients modulo 2 are given by the run length transforms of several famous linear recurrence sequences, such as the positive integers, the fibonacci numbers, the extended lucas numbers, and narayana's cows sequence. in this paper we show that the run length transform of such sequences are 2-regular sequences. this allows us to obtain wu's results and some new ones using the computer program walnut, eliminating the need for long technical proofs.",2023-09-07
"57","a phenomenological approach to interactive knot diagrams","lennart finke, edmund weitz","human-computer interaction","knot diagrams are among the most common visual tools in topology. computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. we introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. this allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. an implementation of this method is provided.",2023-09-01
"58","the cone of mueller matrices","martha takane, j. ivan lopez-reyes, j. othon parra-alcantar","mathematical physics","in the study of polarized light, there are two basic notions: the stokes vectors and the matrices which preserve them, called mueller matrices. the set of stokes vectors forms a cone: the future light cone. in this work we will see that the mueller matrices also form a cone in the vector space of real matrices of size 4x4, called the mueller cone. we obtain some properties of the mueller cone, which in turn will be translated into properties of the stokes vectors. as an application we will give a computational program to calibrate polarimeters by means of the eigenvectors of mueller matrices (ecm). we include computational programs to 1. deduce if a matrix is a mueller matrix, 2. give an approximation of a matrix by a mueller matrix, 3. an approximation of a mueller matrix by mueller invertibles, 4. an approximation of a mueller matrix by a stokes-cone-primitive mueller matrix, see (4.7) and 5. an eigenvalue calibration method. all these programs and implementations can be found in
this https url",2023-08-31
"59","experimenting with chatgpt for spreadsheet formula generation: evidence of risk in ai generated spreadsheets","simon thorne","software engineering","large language models (llm) have become sophisticated enough that complex computer programs can be created through interpretation of plain english sentences and implemented in a variety of modern languages such as python, java script, c++ and spreadsheets. these tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. this paper presents a series of experiments with chatgpt to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where chatgpt has to deduce, infer and problem solve the answer. the results show that in certain circumstances, chatgpt can produce correct spreadsheet formulae with correct reasoning, deduction and inference. however, when information is limited, uncertain or the problem is too complex, the accuracy of chatgpt breaks down as does its ability to reason, infer and deduce. this can also result in false statements and ""hallucinations"" that all subvert the process of creating spreadsheet formulae.",2023-08-31
"60","implementing performance portability of high performance computing programs in the new golden age of chip architecture","weifeng liu, linping wu, xiaowen xu, yuren wang","hardware architecture","as an important goal of high-performance computing, the concept of performance portability has been around for many years. as the failure of moore's law, it is no longer feasible to improve computer performance by simply increasing the number of existing hardware. the innovation of high performance computer is imperative, which makes high-performance computers with multiple architectures coexist in the production environment. for example, current high-performance computing nodes often use co-accelerators such like general-purpose gpus and intel xeon phis to accelerate general-purpose processors. with the flourishing of deep learning, dedicated neural network acceleration chips are also arising. the emergence of co-accelerators with different architectures and their wide application in high-performance computers have challenged the performance portability of programs between high-performance computers with different architectures. this article summarizes the current performance portability technology from the programming model, serial code automatic parallelization, parallel code automatic conversion, etc. at the end of the article, it also summarizes how to use scientific computing function libraries to improve performance and performance portability of a program. different application scenarios need different implementation technologies to get performance portability. program developers choose performance portability solutions for their programs. in fact, they balance programming efficiency and optimization effects under various constraints.",2023-08-26
"61","innovating computer programming pedagogy: the ai-lab framework for generative ai adoption","ethan dickey, andres bejarano, chirayu garg","computers and society","over the last year, the ascent of generative ai (genai) has raised concerns about its impact on core skill development, such as problem-solving and algorithmic thinking, in computer science students. preliminary anonymous surveys show that at least 48.5% of our students use genai for homework. with the proliferation of these tools, the academic community must contemplate the appropriate role of these tools in education. neglecting this might culminate in a phenomenon we term the ""junior-year wall,"" where students struggle in advanced courses due to prior over-dependence on genai. instead of discouraging genai use, which may unintentionally foster covert usage, our research seeks to answer: ""how can educators guide students' interactions with genai to preserve core skill development during their foundational academic years?""
we introduce ""ai-lab,"" a pedagogical framework for guiding students in effectively leveraging genai within core collegiate programming courses. this framework accentuates genai's benefits and potential as a pedagogical instrument. by identifying and rectifying genai's errors, students enrich their learning process. moreover, ai-lab presents opportunities to use genai for tailored support such as topic introductions, detailed examples, corner case identification, rephrased explanations, and debugging assistance. importantly, the framework highlights the risks of genai over-dependence, aiming to intrinsically motivate students towards balanced usage. this approach is premised on the idea that mere warnings of genai's potential failures may be misconstrued as instructional shortcomings rather than genuine tool limitations.
additionally, ai-lab offers strategies for formulating prompts to elicit high-quality genai responses. for educators, ai-lab provides mechanisms to explore students' perceptions of genai's role in their learning experience.",2023-08-23
"62","chatlogo: a large language model-driven hybrid natural-programming language interface for agent-based modeling and programming","john chen, uri wilensky","human-computer interaction","building on papert (1980)'s idea of children talking to computers, we propose chatlogo, a hybrid natural-programming language interface for agent-based modeling and programming. we build upon previous efforts to scaffold abm & p learning and recent development in leveraging large language models (llms) to support the learning of computational programming. chatlogo aims to support conversations with computers in a mix of natural and programming languages, provide a more user-friendly interface for novice learners, and keep the technical system from over-reliance on any single llm. we introduced the main elements of our design: an intelligent command center, and a conversational interface to support creative expression. we discussed the presentation format and future work. responding to the challenges of supporting open-ended constructionist learning of abm & p and leveraging llms for educational purposes, we contribute to the field by proposing the first constructionist llm-driven interface to support computational and complex systems thinking.",2023-08-16
"63","data race detection using large language models","le chen, xianzhong ding, murali emani, tristan vanderbruggen, pei-hung lin, chuanhua liao","machine learning","large language models (llms) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. in this paper, we explore a novel llm-based data race detection approach combining prompting engineering and fine-tuning techniques. we create a dedicated dataset named drb-ml, which is derived from dataracebench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. drb-ml is then used to evaluate representative llms and fine-tune open-source ones. our experiment shows that llms can be a viable approach to data race detection. however, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.",2023-08-15
"64","the ${\cal o}(α_t+α_λ+α_κ)^2$ correction to the $ρ$ parameter and its effect on the w boson mass calculation in the complex nmssm","thi nhung dao, martin gabelmann, m. margarete mühlleitner","high energy physics - phenomenology","we present the prediction of the electroweak $\rho$ parameter and the $w$ boson mass in the cp-violating next-to-minimal supersymmetric extension of the standard model (nmssm) at the two-loop order. the $\rho$ parameter is calculated at the full one-loop and leading and sub-leading two-loop order $\mathcal{o}(\alpha + \alpha_t\alpha_s + \left(\alpha_t+\alpha_\lambda+\alpha_\kappa\right)^2)$. the new $\delta \rho$ prediction is incorporated into a prediction of $m_w$ via a full supersymmetric (susy) one-loop calculation of $\delta r$. furthermore, we include all known state-of-the-art sm higher-order corrections to $\delta r$. by comparing results for $\delta \rho$ obtained using on-shell (os) and $\overline{\mathrm{dr}}$ renormalization conditions in the top/stop sector, we find that the scheme uncertainty is reduced at one-loop order by 55%, at two-loop $\mathcal{o}(\alpha_s\alpha_t)$ by 22%, and at two-loop $\mathcal{o}(\alpha_t+\alpha_\kappa+\alpha_\lambda)^2$ by 16%, respectively. the influence of the two-loop results on the $m_w$ mass prediction is found to be sub-leading. the new calculation is made public in the computer program $\mathrm{\tt nmssmcalc}$. we perform an extensive comparison in the $w$-mass, higgs boson mass and the muon anomalous magnetic moment prediction between our calculation and three other publicly available tools and find very good agreement provided that the input parameters and renormalization scales are treated in the same way. finally, we study the impact of the cp-violating phases on the $w$-mass prediction which is found to be smaller than the overall size of the susy corrections.",2023-08-08
"65","synthesizing programmatic policies with actor-critic algorithms and relu networks","spyros orfanos, levi h. s. lelis","machine learning","programmatically interpretable reinforcement learning (pirl) encodes policies in human-readable computer programs. novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. most of such pirl algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. in this paper, we show that such pirl-specific algorithms are not needed, depending on the language used to encode the programmatic policies. this is because one can use actor-critic algorithms to directly obtain a programmatic policy. we use a connection between relu neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. this translation from relu networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and pid operations. empirical results on several control problems show that this translation approach is capable of learning short and effective policies. moreover, the translated policies are at least competitive and often far superior to the policies pirl algorithms synthesize.",2023-08-04
"66","should we trust web-scraped data?","jens foerderer","general economics","the increasing adoption of econometric and machine-learning approaches by empirical researchers has led to a widespread use of one data collection method: web scraping. web scraping refers to the use of automated computer programs to access websites and download their content. the key argument of this paper is that naïve web scraping procedures can lead to sampling bias in the collected data. this article describes three sources of sampling bias in web-scraped data. more specifically, sampling bias emerges from web content being volatile (i.e., being subject to change), personalized (i.e., presented in response to request characteristics), and unindexed (i.e., abundance of a population register). in a series of examples, i illustrate the prevalence and magnitude of sampling bias. to support researchers and reviewers, this paper provides recommendations on anticipating, detecting, and overcoming sampling bias in web-scraped data.",2023-08-04
"67","blueprinting quantum computing systems","simon j. devitt","quantum physics","the development of quantum computing systems has been a staple of academic research since the mid-1990s when the first proposal for physical platforms were proposed using nuclear magnetic resonance and ion-trap hardware. these first proposals were very basic, essentially consisting of identifying a physical qubit (two-level quantum system) that could be isolated and controlled to achieve universal quantum computation. over the past thirty years, the nature of quantum architecture design has changed significantly and the scale of investment, groups and companies involved in building quantum computers has increased exponentially. architectural design for quantum computers examines systems at scale: fully error-corrected machines, potentially consisting of millions if not billions of physical qubits. these designs increasingly act as blueprints for academic groups and companies and are becoming increasingly more detailed, taking into account both the nature and operation of the physical qubits themselves and also peripheral environmental and control infrastructure that is required for each physical system. in this paper, several architectural structures that i have worked on will be reviewed, each of which has been adopted by either a national quantum computing program or a quantum startup.",2023-07-29
"68","wavjourney: compositional audio creation with large language models","xubo liu, zhongkai zhu, haohe liu, yi yuan, meng cui, qiushi huang, jinhua liang, yin cao, qiuqiang kong, mark d. plumbley, wenwu wang","sound","despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. however, real-world audio creation aims to generate harmonious audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. we present wavjourney, a novel framework that leverages large language models (llms) to connect various audio models for audio creation. wavjourney allows users to create storytelling audio content with diverse audio elements simply from textual descriptions. specifically, given a text instruction, wavjourney first prompts llms to generate an audio script that serves as a structured semantic representation of audio elements. the audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. the computer program is then executed to obtain a compositional and interpretable solution for audio creation. experimental results suggest that wavjourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-to-audio generation benchmarks. additionally, we introduce a new multi-genre story benchmark. subjective evaluations demonstrate the potential of wavjourney in crafting engaging storytelling audio content from text. we further demonstrate that wavjourney can facilitate human-machine co-creation in multi-round dialogues. to foster future research, the code and synthesized audio are available at: this https url.",2023-07-26
"69","advanced monte carlo simulation techniques to study polymers under equilibrium conditions","monika angwani, tushar mahendrakar, kaustubh rane","soft condensed matter","the advances in materials and biological sciences have necessitated the use of molecular simulations to study polymers. the markov chain monte carlo simulations enable the sampling of relevant microstates of polymeric systems by traversing paths that are impractical in molecular dynamics simulations. several advances in applying monte carlo simulations to polymeric systems have been reported in recent decades. the proposed methods address sampling challenges encountered in studying different aspects of polymeric systems. tracking the above advances has become increasingly challenging due to the extensive literature generated in the field. moreover, the incorporation of new methods in the existing monte carlo simulation packages is cumbersome due to their complexity. identifying the foundational algorithms that are common to different methods can significantly ease their implementation and make them accessible to the broader simulation community. the present chapter classifies the monte carlo methods for polymeric systems based on their objectives and standard features of their algorithms. we begin the article by providing an overview of advanced monte carlo techniques used for polymeric systems and their specific applications. we then classify the above techniques into two broad categories: 1) monte carlo moves and 2) advanced sampling schemes. the former category is further divided to distinguish the monte carlo moves in the canonical and other ensembles. the advanced sampling schemes attempt to improve monte carlo sampling via approaches other than monte carlo moves. we use the above classification to identify common features of the methods and derive general expressions that explain their implementation. such a strategy can help readers select the methods that are suitable for their study and develop computer programs that can be easily modified to implement new methods.",2023-07-21
"70","visual flow-based programming plugin for brain computer interface in computer-aided design","tong bill xu, saleh kalantari","human-computer interaction","over the last half century, the main application of brain computer interfaces, bcis has been controlling wheelchairs and neural prostheses or generating text or commands for people with restricted mobility. there has been very limited attention in the field to applications for computer aided design, despite the potential of bcis to provide a new form of environmental interaction. in this paper we introduce the development and application of neuron, a novel bci tool that enables designers with little experience in neuroscience or computer programming to gain access to neurological data, along with established metrics relevant to design, create bci interaction prototypes, both with digital onscreen objects and physical devices, and evaluate designs based on neurological information and record measurements for further analysis. after discussing the bci tool development, the article presents its capabilities through two case studies, along with a brief evaluation of the tool performance and a discussion of implications, limitations, and future improvement.",2023-07-20
"71","comparing with python: text analysis in stata","xiangtai zuo (shutter zor)","methodology","text analysis is the process of constructing structured data from unstructured textual content, usually implemented in python. in terms of the principles of text analysis, a computer program with the ability to read a file and match it with a regular expression is all that is needed for basic text analysis. however, few researchers have used stata as their main text analysis tool. in this paper, i will take a step-by-step approach to the practical process, giving examples of how text analysis can be performed with stata, and comparing the code and running time with python.",2023-07-19
"72","glamour muscles: why having a body is not what it means to be embodied","shawn l. beaulieu, sam kriegman","artificial intelligence","embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines. proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space. however, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language. rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them. these include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible. to be theoretically or practically useful beyond the creation of niche tools, we argue that ""embodiment"" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body$\unicode{x2013}$that is, to participate in what it means to $\textit{constitute}$ a given body. it follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.",2023-07-17
"73","are large language models a threat to digital public goods? evidence from activity on stack overflow","maria del rio-chanona, nadzeya laurentsyeva, johannes wachs","social and information networks","large language models like chatgpt efficiently provide users with information about various topics, presenting a potential substitute for searching the web and asking people for help online. but since users interact privately with the model, these models may drastically reduce the amount of publicly available human-generated data and knowledge resources. this substitution can present a significant problem in securing training data for future models. in this work, we investigate how the release of chatgpt changed human-generated open data on the web by analyzing the activity on stack overflow, the leading online q\&a platform for computer programming. we find that relative to its russian and chinese counterparts, where access to chatgpt is limited, and to similar forums for mathematics, where chatgpt is less capable, activity on stack overflow significantly decreased. a difference-in-differences model estimates a 16\% decrease in weekly posts on stack overflow. this effect increases in magnitude over time, and is larger for posts related to the most widely used programming languages. posts made after chatgpt get similar voting scores than before, suggesting that chatgpt is not merely displacing duplicate or low-quality content. these results suggest that more users are adopting large language models to answer questions and they are better substitutes for stack overflow for languages for which they have more training data. using models like chatgpt may be more efficient for solving certain programming problems, but its widespread adoption and the resulting shift away from public exchange on the web will limit the open data people and models can learn from in the future.",2023-07-14
"74","exact generalized turán number for $k_3$ versus suspension of $p_4$","sayan mukherjee","combinatorics","let $p_4$ denote the path graph on $4$ vertices. the suspension of $p_4$, denoted by $\widehat p_4$, is the graph obtained via adding an extra vertex and joining it to all four vertices of $p_4$. in this note, we demonstrate that for $n\ge 8$, the maximum number of triangles in any $n$-vertex graph not containing $\widehat p_4$ is $\left\lfloor n^2/8\right\rfloor$. our method uses simple induction along with computer programming to prove a base case of the induction hypothesis.",2023-07-10
"75","multi-intent detection in user provided annotations for programming by examples systems","nischal ashok kumar, nitin gupta, shanmukha guttula, hima patel","artificial intelligence","in mapping enterprise applications, data mapping remains a fundamental part of integration development, but its time consuming. an increasing number of applications lack naming standards, and nested field structures further add complexity for the integration developers. once the mapping is done, data transformation is the next challenge for the users since each application expects data to be in a certain format. also, while building integration flow, developers need to understand the format of the source and target data field and come up with transformation program that can change data from source to target format. the problem of automatic generation of a transformation program through program synthesis paradigm from some specifications has been studied since the early days of artificial intelligence (ai). programming by example (pbe) is one such kind of technique that targets automatic inferencing of a computer program to accomplish a format or string conversion task from user-provided input and output samples. to learn the correct intent, a diverse set of samples from the user is required. however, there is a possibility that the user fails to provide a diverse set of samples. this can lead to multiple intents or ambiguity in the input and output samples. hence, pbe systems can get confused in generating the correct intent program. in this paper, we propose a deep neural network based ambiguity prediction model, which analyzes the input-output strings and maps them to a different set of properties responsible for multiple intent. users can analyze these properties and accordingly can provide new samples or modify existing samples which can help in building a better pbe system for mapping enterprise applications.",2023-07-08
"76","pushing the limits of machine design: automated cpu design with ai","shuyao cheng, pengwei jin, qi guo, zidong du, rui zhang, yunhao tian, xing hu, yongwei zhao, yifan hao, xiangtao guan, husheng han, zhengyue zhao, ximing liu, ling li, xishan zhang, yuejie chu, weilong mao, tianshi chen, yunji chen","artificial intelligence","design activity -- constructing an artifact description satisfying given goals and constraints -- distinguishes humanity from other animals and traditional machines, and endowing machines with design abilities at the human level or beyond has been a long-term pursuit. though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (ai) techniques, the search space for designing such objects is relatively small, and thus, ""can machines design like humans?"" remains an open question. to explore the boundary of machine design, here we present a new ai approach to automatically design a central processing unit (cpu), the brain of a computer, and one of the world's most intricate devices humanity have ever designed. this approach generates the circuit logic, which is represented by a graph structure called binary speculation diagram (bsd), of the cpu design from only external input-output observations instead of formal program code. during the generation of bsd, monte carlo-based expansion and the distance of boolean functions are used to guarantee accuracy and efficiency, respectively. by efficiently exploring a search space of unprecedented size 10^{10^{540}}, which is the largest one of all machine-designed objects to our best knowledge, and thus pushing the limits of machine design, our approach generates an industrial-scale risc-v cpu within only 5 hours. the taped-out cpu successfully runs the linux operating system and performs comparably against the human-designed intel 80486sx cpu. in addition to learning the world's first cpu only from input-output observations, which may reform the semiconductor industry by significantly reducing the design cycle, our approach even autonomously discovers human knowledge of the von neumann architecture.",2023-06-21
"77","bianchi period polynomials: hecke action and congruences","lewis combes","number theory","let $\gamma$ be a bianchi group associated to one of the five euclidean imaginary quadratic fields. we show that the space of weight $k$ period polynomials for $\gamma$ is ``dual'' to the space of weight $k$ modular symbols for $\gamma$, reflecting the duality between the first and second cohomology groups of bianchi groups. using this result, we describe the action of hecke operators on the space of period polynomials for $\gamma$ via the heilbronn matrices.
in the second part of the paper, we numerically investigate congruences between level 1 bianchi eigenforms via computer programs which implement the hecke action on spaces of bianchi period polynomials. computations with the hecke action are used to indicate moduli of congruences between the underlying bianchi forms; we then prove the congruences using the period polynomials. from this we find congruences between genuine bianchi modular forms and both a base-change bianchi form and an eisenstein series. we believe these congruences are the first of their kind in the literature.",2023-06-19
"78","stacking of hyperparameter tuned models for tagging coding problems","sathya krishnan ts, s. lakshmana pandian, p. shunmugapriya","machine learning","coding problems are problems that require a solution in the form of a computer program. coding problems are popular among students and professionals as it enhances their skills and career opportunities. an ai system that would help those who practice coding problems would be highly useful and there is a huge potential for such a system. in this work, we propose a model which uses stacking of hyperparameter tuned boosting models to achieve impressive metric scores of 77.8% accuracy and 0.815 pr-auc on the dataset that was scraped from codeforces and leetcode. we open source the dataset and the models developed for this work.",2023-06-16
"79","chatbots to chatgpt in a cybersecurity space: evolution, vulnerabilities, attacks, challenges, and future recommendations","attia qammar, hongmei wang, jianguo ding, abdenacer naouri, mahmoud daneshmand, huansheng ning","cryptography and security","chatbots shifted from rule-based to artificial intelligence techniques and gained traction in medicine, shopping, customer services, food delivery, education, and research. openai developed chatgpt blizzard on the internet as it crossed one million users within five days of its launch. however, with the enhanced popularity, chatbots experienced cybersecurity threats and vulnerabilities. this paper discussed the relevant literature, reports, and explanatory incident attacks generated against chatbots. our initial point is to explore the timeline of chatbots from eliza (an early natural language processing computer program) to gpt-4 and provide the working mechanism of chatgpt. subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. besides, we investigated the chatgpt, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and lolbins. furthermore, the history of cyberattacks and vulnerabilities exploited by cybercriminals are discussed, particularly considering the risk and vulnerabilities in chatgpt. addressing these threats and vulnerabilities requires specific strategies and measures to reduce the harmful consequences. therefore, the future directions to address the challenges were presented.",2023-05-29
"80","plan b: new ${z^\prime}$ models for $b\rightarrow sl^+l^-$ anomalies","ben allanach, anna mullin","high energy physics - phenomenology","measurements of $b \rightarrow s \mu^+ \mu^-$ transitions indicate that there may be a new physics field coupling to di-muon pairs associated with the $b$ to $s$ flavour transition. including the 2022 lhcb reanalysis of $r_k$ and $r_{k^\ast}$, one infers that there may also be associated new physics in $b\rightarrow e^+ e^-$ transitions. here, we examine the extent of the statistical preference for $z^\prime$ models coupling to di-electron pairs taking into account the relevant constraints, in particular from experiments at lep-2. we identify an anomaly-free set of models which interpolates between the $z^\prime$ not coupling to electrons at all, to one in which there is an equal $z^\prime$ coupling to muons and electrons (but where in all models in the set, the $z^\prime$ boson can mediate $b\rightarrow \mu^+ \mu^-$ transitions). a $3b_3-l_e-2l_\mu$ model provides a close-to-optimal fit to the pertinent measurements along the line of interpolation. we have (re-)calculated predictions for the relevant lep-2 observables in terms of dimension-6 smeft operators and put them into the ${\tt flavio2.3.3}$ computer program, so that they are available for global fits.",2023-06-14
"81","minimally comparing relational abstract domains","kenny ballou, elena sherman","logic in computer science","value-based static analysis techniques express computed program invariants as logical formula over program variables. researchers and practitioners use these invariants to aid in software engineering and verification tasks. when selecting abstract domains, practitioners weigh the cost of a domain against its expressiveness. however, an abstract domain's expressiveness tends to be stated in absolute terms; either mathematically via the sub-polyhedra the domain is capable of describing, empirically using a set of known properties to verify, or empirically via logical entailment using the entire invariant of the domain at each program point. due to carry-over effects, however, the last technique can be problematic because it tends to provide a simplistic and imprecise comparisons.
we address limitations of comparing, in general, abstract domains via logical entailment in this work. we provide a fixed-point algorithm for including the minimally necessary variables from each domain into the compared formula. furthermore, we empirically evaluate our algorithm, comparing different techniques of widening over the zones domain and comparing zones to an incomparable relational predicates domain. our empirical evaluation of our technique shows an improved granularity of comparison. it lowered the number of more precise invariants when comparing analysis techniques, thus, limiting the prevalent carry-over effects. moreover, it removed undecidable invariants and lowered the number of incomparable invariants when comparing two incomparable relational abstract domains.",2023-05-25
"82","statistics of feynman amplitudes in $ϕ^4$-theory","paul-hermann balduf","high energy physics - theory","the amplitude of subdivergence-free logarithmically divergent feynman graphs in $\phi^4$-theory in 4 spacetime dimensions is given by a single number, the feynman period. we numerically compute the periods of 1.3 million completed graphs, this represents more than 33 million graphs contributing to the beta function. our data set includes all primitive graphs up to 13 loops, and non-complete samples up to 18 loops, with an accuracy of ca. 4 significant digits. we implement all known symmetries of the period in a new computer program and count them up to 14 loops. combining the symmetries, we discover relations between periods that had been overlooked earlier. all expected symmetries are respected by the numerical values of periods. we examine the distribution of the numerically computed feynman periods. we confirm the leading asymptotic growth of the average period with growing loop order, up to a factor of 2. at high loop order, a limiting distribution is reached for the amplitudes near the mean. a small class of graphs, most notably the zigzags, grows significantly faster than the mean and causes the limiting distribution to have divergent moments even when normalized to unit mean. we examine the relation between the period and various properties of the underlying graphs. we confirm the strong correlation with the hepp bound, the martin invariant, and the number of 6-edge cuts. we find that, on average, the amplitude of planar graphs is significantly larger than that of non-planar graphs, irrespective of $o(n)$ symmetry. we estimate the primitive contribution to the 18-loop beta function of the $o(n)$-symmetric theory. we find that primitive graphs constitute a large part of the beta function in ms for $l\rightarrow \infty$ loops. the relative contribution of planar graphs increases with growing $n$ and decreases with growing loop order $l$.",2023-05-22
"83","qhdl: a low-level circuit description language for quantum computing","gilbert netzer, stefano markidis","emerging technologies","this paper proposes a descriptive language called qhdl, akin to vhdl, to program gate-based quantum computing systems. unlike other popular quantum programming languages, qhdl targets low-level quantum computing programming and aims to provide a common framework for programming fpgas and gate-based quantum computing systems. the paper presents an initial implementation and design principles of the qhdl framework, including a compiler and quantum computer simulator. we discuss the challenges of low-level integration of streaming models and quantum computing for programming fpgas and gate-based quantum computing systems.",2023-05-16
"84","automatic differentiation in prolog","tom schrijvers, birthe van den berg, fabrizio riguzzi","programming languages","automatic differentiation (ad) is a range of algorithms to compute the numeric value of a function's (partial) derivative, where the function is typically given as a computer program or abstract syntax tree. ad has become immensely popular as part of many learning algorithms, notably for neural networks. this paper uses prolog to systematically derive gradient-based forward- and reverse-mode ad variants from a simple executable specification: evaluation of the symbolic derivative. along the way we demonstrate that several prolog features (dcgs, co-routines) contribute to the succinct formulation of the algorithm. we also discuss two applications in probabilistic programming that are enabled by our prolog algorithms. the first is parameter learning for the sum-product loop language and the second consists of both parameter learning and variational inference for probabilistic logic programming.",2023-05-13
"85","understanding automatic differentiation pitfalls","jan hückelheim, harshitha menon, william moses, bruce christianson, paul hovland, laurent hascoët","numerical analysis","automatic differentiation, also known as backpropagation, ad, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. sometimes, however, the derivatives computed by ad could be interpreted as incorrect. these pitfalls occur systematically across tools and approaches. in this paper we broadly categorize problematic usages of ad and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. we also review debugging techniques and their effectiveness in these situations. with this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from ad tools.",2023-05-12
"86","humans are still better than chatgpt: case of the ieeextreme competition","anis koubaa, basit qureshi, adel ammar, zahid khan, wadii boulila, lahouari ghouti","software engineering","since the release of chatgpt, numerous studies have highlighted the remarkable performance of chatgpt, which often rivals or even surpasses human capabilities in various tasks and domains. however, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for chatgpt, specifically in the domain of computer programming. we utilize the ieeextreme challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. to conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct ieeextreme editions, using three major programming languages: python, java, and c++. our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over chatgpt in certain aspects of problem-solving within the programming context. in fact, we found that the average score obtained by chatgpt on the set of ieeextreme programming problems is 3.9 to 5.8 times lower than the average human score, depending on the programming language. this paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for ai-based language models like chatgpt.",2023-05-10
"87","can chatgpt pass an introductory level functional language programming course?","chuqin geng, yihan zhang, brigitte pientka, xujie si","computers and society","the recent introduction of chatgpt has drawn significant attention from both industry and academia due to its impressive capabilities in solving a diverse range of tasks, including language translation, text summarization, and computer programming. its capability for writing, modifying, and even correcting code together with its ease of use and access is already dramatically impacting computer science education. this paper aims to explore how well chatgpt can perform in an introductory-level functional language programming course. in our systematic evaluation, we treated chatgpt as one of our students and demonstrated that it can achieve a grade b- and its rank in the class is 155 out of 314 students overall. our comprehensive evaluation provides valuable insights into chatgpt's impact from both student and instructor perspectives. additionally, we identify several potential benefits that chatgpt can offer to both groups. overall, we believe that this study significantly clarifies and advances our understanding of chatgpt's capabilities and potential impact on computer science education.",2023-04-29
"88","introduction to the theory of digital spaces","alexander v. evako","general topology","this book provides an introduction to the theory of digital (molecular) spaces (tds). digital spaces are combinatorial models of continuous spaces. tds is one of alternative branches of digital topology that studies constructing and modifying 2, 3 and n-dimensional digital image arrays in a computer and its memory. demands for mathematical theory of multidimensional spaces built of finite number of points appeared in connection with the use of many dimensional images in computer programs. this work presents, as examples, digital models of n-dimensional euclidean spaces, n-dimensional spheres, a torus, a projective plane, etc. methods of tms can work successfully in such areas as biology, chemistry, industry, medicine, etc. the book is organized as follows. digital models of continuous spaces are the intersection graphs of special locally centered lamp covers of continuous spaces. digital models are represented by graphs, algebraic matrices and a set of unit cubes in n-dimensional euclidean space. the main mathematical characteristics of digital models of continuous spaces, such as the dimension, the euler characteristic, the homology groups and others, are the same as those of their continuous counterparts. contractible transformations of digital spaces, that change the number of elements, do not change the mathematical characteristics and properties of digital spaces. the main goal is to construct digital models of continuous spaces and to explain how to use the new methods to work with mathematical objects. the emphasis is on introducing the readers to the basics and conceptual understanding of what tds is.",2023-04-15
"89","a survey of methods for converting unstructured data to csg models","pierre-alain fayolle, markus friedrich","graphics","the goal of this document is to survey existing methods for recovering csg representations from unstructured data such as 3d point-clouds or polygon meshes. we review and discuss related topics such as the segmentation and fitting of the input data. we cover techniques from solid modeling and cad for polyhedron to csg and b-rep to csg conversion. we look at approaches coming from program synthesis, evolutionary techniques (such as genetic programming or genetic algorithm), and deep learning methods. finally, we conclude with a discussion of techniques for the generation of computer programs representing solids (not just csg models) and higher-level representations (such as, for example, the ones based on sketch and extrusion or feature based operations).",2023-05-02
"90","thinking beyond chatbots' threat to education: visualizations to elucidate the writing and coding process","badri adhikari","computers and society","the landscape of educational practices for teaching and learning languages has been predominantly centered around outcome-driven approaches. the recent accessibility of large language models has thoroughly disrupted these approaches. as we transform our language teaching and learning practices to account for this disruption, it is important to note that language learning plays a pivotal role in developing human intelligence. writing and computer programming are two essential skills integral to our education systems. what and how we write shapes our thinking and sets us on the path of self-directed learning. while most educators understand that `process' and `product' are both important and inseparable, in most educational settings, providing constructive feedback on a learner's formative process is challenging. for instance, it is straightforward in computer programming to assess whether a learner-submitted code runs. however, evaluating the learner's creative process and providing meaningful feedback on the process can be challenging. to address this long-standing issue in education (and learning), this work presents a new set of visualization tools to summarize the inherent and taught capabilities of a learner's writing or programming process. these interactive process visualizations (pvs) provide insightful, empowering, and personalized process-oriented feedback to the learners. the toolbox is ready to be tested by educators and learners and is publicly available at this http url. focusing on providing feedback on a learner's process--from self, peers, and educators--will facilitate learners' ability to acquire higher-order skills such as self-directed learning and metacognition.",2023-04-25
"91","a diagrammatic method to compute the effective hamiltonian of driven nonlinear oscillators","xu xiao, jayameenakshi venkatraman, rodrigo g. cortiñas, shoumik chowdhury, michel h. devoret","quantum physics","in this work, we present a new method, based on feynman-like diagrams, for computing the effective hamiltonian of driven nonlinear oscillators. the pictorial structure associated with each diagram corresponds directly to a hamiltonian term, the prefactor of which involves a simple counting of topologically equivalent diagrams. we also leverage the algorithmic simplicity of our scheme in a readily available computer program that generates the effective hamiltonian to arbitrary order. at the heart of our diagrammatic method is a novel canonical perturbation expansion developed in phase space to capture the quantum nonlinear dynamics. a merit of this expansion is that it reduces to classical harmonic balance in the limit of $\hbar\rightarrow0$. our method establishes the foundation of the dynamic control of quantum systems with the precision needed for future quantum machines. we demonstrate its value by treating five examples from the field of superconducting circuits. these examples involve an experimental proposal for the hamiltonian stabilization of a three-legged schrödinger cat, modeling of energy renormalization phenomena in superconducting circuits experiments, a comprehensive characterization of multiphoton resonances in a driven transmon, a proposal for an novel inductively shunted transmon circuit, and a characterization of classical ultra-subharmonic bifurcation in driven oscillators. lastly, we benchmark the performance of our method by comparing it with experimental data and exact floquet numerical diagonalization.",2023-04-26
"92","ai-assisted coding: experiments with gpt-4","russell a poldrack, thomas lu, gašper beguš","artificial intelligence","artificial intelligence (ai) tools based on large language models have acheived human-level performance on some computer programming tasks. we report several experiments using gpt-4 to generate computer code. these experiments demonstrate that ai code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. we also demonstrate that gpt-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that gpt-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. these findings suggest that while ai coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.",2023-04-25
"93","chatgpt (feb 13 version) is a chinese room","maurice ht ling","computation and language","chatgpt has gained both positive and negative publicity after reports suggesting that it is able to pass various professional and licensing examinations. this suggests that chatgpt may pass turing test in the near future. however, a computer program that passing turing test can either mean that it is a chinese room or artificially conscious. hence, the question of whether the current state of chatgpt is more of a chinese room or approaching artificial consciousness remains. here, i demonstrate that the current version of chatgpt (feb 13 version) is a chinese room. despite potential evidence of cognitive connections, chatgpt exhibits critical errors in causal reasoning. at the same time, i demonstrate that chatgpt can generate all possible categorical responses to the same question and response with erroneous examples; thus, questioning its utility as a learning tool. i also show that chatgpt is capable of artificial hallucination, which is defined as generating confidently wrong replies. it is likely that errors in causal reasoning leads to hallucinations. more critically, chatgpt generates false references to mimic real publications. therefore, its utility is cautioned.",2023-02-19
"94","program comprehension does not primarily rely on the language centers of the human brain","shashank srikant, anna a. ivanova, yotaro sueoka, hope h. kean, riva dhamala, evelina fedorenko, marina u. bers, una-may o'reilly","software engineering","our goal is to identify brain regions involved in comprehending computer programs. we use functional magnetic resonance imaging (fmri) to investigate two candidate systems of brain regions which may support this -- the multiple demand (md) system, known to respond to a range of cognitively demanding tasks, and the language system (ls), known to primarily respond to language stimuli. we devise experiment conditions to isolate the act of code comprehension, and employ a state-of-the-art method to locate brain systems of interest. we administer these experiments in python (24 participants) and scratch jr. (19 participants) - which provides a visual interface to programming, thus eliminating the effect of text in code comprehension. from this robust experiment setup, we find that the language system is not consistently involved in code comprehension, while the md is. further, we find no other brain regions beyond those in the md to be responsive to code. we also find that variable names, the control flow used in the program, and the types of operations performed do not affect brain responses. we discuss the implications of our findings on the software engineering and cs education communities.",2023-04-11
"95","llms can generate robotic scripts from goal-oriented instructions in biological laboratory automation","takashi inagaki, akari kato, koichi takahashi, haruka ozaki, genki n. kanda","quantitative methods","the use of laboratory automation by all researchers may substantially accelerate scientific activities by humans, including those in the life sciences. however, computer programs to operate robots should be written to implement laboratory automation, which requires technical knowledge and skills that may not be part of a researcher's training or expertise. in the last few years, there has been remarkable development in large language models (llms) such as gpt-4, which can generate computer codes based on natural language instructions. in this study, we used llms, including gpt-4, to generate scripts for robot operations in biological experiments based on ambiguous instructions. gpt-4 successfully generates scripts for ot-2, an automated liquid-handling robot, from simple instructions in natural language without specifying the robotic actions. conventionally, translating the nuances of biological experiments into low-level robot actions requires researchers to understand both biology and robotics, imagine robot actions, and write robotic scripts. our results showed that gpt-4 can connect the context of biological experiments with robot operation through simple prompts with expert-level contextual understanding and inherent knowledge. replacing robot script programming, which is a tedious task for biological researchers, with natural-language llm instructions that do not consider robot behavior significantly increases the number of researchers who can benefit from automating biological experiments.",2023-04-18
"96","a neural lambda calculus: neurosymbolic ai meets the foundations of computing and functional programming","joão flach, luis c. lamb","machine learning","over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. to study the capabilities of neural networks in the symbolic ai domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. the latter is known to be too complex a task for neural networks. therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. in this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. to do so, we propose a different approach. instead of using an imperative programming language, with complex structures, we use the lambda calculus ({\lambda}-calculus), a simple, but turing-complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. we will introduce the use of integrated neural learning and lambda calculi formalization. finally, we explore execution of a program in {\lambda}-calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. keywords: machine learning, lambda calculus, neurosymbolic ai, neural networks, transformer model, sequence-to-sequence models, computational models",2023-04-18
"97","magnitude of arithmetic scalar and matrix categories","steve huntsman","category theory","we develop tools for explicitly constructing categories enriched over generating data and that compose via ordinary scalar and matrix arithmetic arithmetic operations. we characterize meaningful size maps, weightings, and magnitude that reveal features analogous to outliers that these same notions have previously been shown to reveal in the context of metric spaces. throughout, we provide examples of such ""outlier detection"" relevant to the analysis of computer programs, neural networks, cyber-physical systems, and networks of communications channels.",2023-04-17
"98","dialogue games for benchmarking language understanding: motivation, taxonomy, strategy","david schlangen","computation and language","how does one measure ""ability to understand language""? if it is a person's ability that is being measured, this is a question that almost never poses itself in an unqualified manner: whatever formal test is applied, it takes place on the background of the person's language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). computer programs do not have this background. what does that mean for the applicability of formal tests of language understanding? i argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of ""artificial language understanding"". to do such tests systematically, i propose to use ""dialogue games"" -- constructed activities that provide a situational embedding for language use. i describe a taxonomy of dialogue game types, linked to a model of underlying capabilites that are tested, and thereby giving an argument for the \emph{construct validity} of the test. i close with showing how the internal structure of the taxonomy suggests an ordering from more specialised to more general situational language understanding, which potentially can provide some strategic guidance for development in this field.",2023-04-14
"99","emergent autonomous scientific research capabilities of large language models","daniil a. boiko, robert macknight, gabe gomes","chemical physics","transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. in this paper, we present an intelligent agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. we showcase the agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.",2023-04-11
"100","a native measurement-based qaoa algorithm, applied to the max k-cut problem","massimiliano proietti, filippo cerocchi, massimiliano dispenza","quantum physics","photonic quantum computers, programmed within the framework of the measurement-based quantum computing (mbqc), currently concur with gate-based platforms in the race towards useful quantum advantage, and some algorithms emerged as main candidates to reach this goal in the near term. yet, the majority of these algorithms are only expressed in the gate-based model of computation, which is incompatible with photonic platforms. methods to translate gate-based algorithms into the mbqc framework exist, but they are not always optimal in terms of resource cost. in our work, we propose an mbqc algorithm to run the quantum approximate optimization algorithm (qaoa). furthermore, we apply the mbqc-qaoa algorithm to the max $k$-cut problem, working for all values of $k$, expressing the cost hamiltonian and its constraints in a form easily implementable in the mbqc model. we conclude analyzing the resource-cost of our algorithm, compared to the case of translating a gate-based qaoa algorithm into mbqc rules showing up to a 30-fold improvement. with our work, we contribute to close the gap between gate-based and mbqc near-term algorithms, a gap not reflecting the current status of the hardware development.",2023-04-07
"101","gi software with fewer data cache misses","william b. langdon, justyna petke, aymeric blot, david clark","neural and evolutionary computing","by their very name caches are often overlooked and yet play a vital role in the performance of modern and indeed future hardware. using magpie (machine automated general performance improvement via evolution of software) we show genetic improvement gi can reduce the cache load of existing computer programs. operating on lines of c and c++ source code using local search, magpie can generate new functionally equivalent variants which generate fewer l1 data cache misses. cache miss reduction is tested on two industrial open source programs (google's open location code olc and uber's hexagonal hierarchical spatial index h3) and two 2d photograph image processing tasks, counting pixels and opencv's seeds segmentation algorithm.
magpie's patches functionally generalise. in one case they reduce data misses on the highest performance l1 cache dramatically by 47 percent.",2023-04-06
"102","hotgp -- higher-order typed genetic programming","matheus campos fernandes, fabrício olivetti de frança, emilio francesquini","neural and evolutionary computing","program synthesis is the process of generating a computer program following a set of specifications, which can be a high-level description of the problem and/or a set of input-output examples. the synthesis can be modeled as a search problem in which the search space is the set of all the programs valid under a grammar. as the search space is vast, brute force is usually not viable and search heuristics, such as genetic programming, also have difficulty navigating it without any guidance. in this paper we present hotgp, a new genetic programming algorithm that synthesizes pure, typed, and functional programs. hotgp leverages the knowledge provided by the rich data-types associated with the specification and the built-in grammar to constrain the search space and improve the performance of the synthesis. the grammar is based on haskell's standard base library (the synthesized code can be directly compiled using any standard haskell compiler) and includes support for higher-order functions, $\lambda$-functions, and parametric polymorphism. experimental results show that, when compared to $6$ state-of-the-art algorithms using a standard set of benchmarks, hotgp is competitive and capable of synthesizing the correct programs more frequently than any other of the evaluated algorithms.",2023-04-06
"103","classification and enumeration of lattice polygons in a disc","qiuyue liu, yuqin zhang, zhanyuan cai","metric geometry","in 1980, v. i. arnold studied the classification problem for convex lattice polygons of given area. since then, this problem and its analogues have been studied by many authors, including $\mathrm{b\acute{a}r\acute{a}ny}$, lagarias, pach, santos, ziegler and zong. recently, zong proposed two computer programs to prove hadwiger's covering conjecture and borsuk's partition problem, respectively, based on enumeration of the convex lattice polytopes contained in certain balls. for this purpose, similar to $\mathrm{b\acute{a}r\acute{a}ny}$ and pach's work on volume and liu and zong's work on cardinality, we obtain bounds on the number of non-equivalent convex lattice polygons in a given disc. furthermore, we propose an algorithm to enumerate these convex lattice polygons.",2023-03-27
"104","openscv: an open hierarchical taxonomy for smart contract vulnerabilities","fernando richter vidal, naghmeh ivaki, nuno laranjeiro","cryptography and security","smart contracts are nowadays at the core of most blockchain systems, as they specify and allow an agreement between entities that wish to perform a transaction. as any computer program, smart contracts are subject to the presence of residual faults, including severe security vulnerabilities, which require that the vulnerable contract is terminated in the blockchain. in this context, research began to be developed to prevent the deployment of smart contract holding vulnerabilities, mostly in the form of vulnerability detection tools. along with these efforts, several and heterogeneous vulnerability classification schemes arised (e.g., most notably dasp and swc). at the time of writing, these are mostly outdated initiatives, despite the fact that smart contract vulnerabilities are continuously being discovered and the associated rich information being mostly disregarded. in this paper, we propose openscv, a new and open hierarchical taxonomy for smart contract vulnerabilities, which is open to community contributions and matches the current state of the practice, while being prepared to handle future modifications and evolution. the taxonomy was built based on the analysis of research on vulnerability classification, community-maintained classification schemes, and research on smart contract vulnerability detection. we show how openscv covers the announced detection ability of current vulnerability detection tools, and highlight its usefulness as a resource in smart contract vulnerability research.",2023-03-25
"105","the first computer program","raúl rojas","general literature","in 1837, the first computer program in history was sketched by the renowned mathematician and inventor charles babbage. it was a program for the analytical engine. the program consists of a sequence of arithmetical operations and the necessary variable addresses (memory locations) of the arguments and the result, displayed in tabular fashion, like a program trace. the program computes the solutions for a system of two linear equations in two unknowns.",2023-03-24
"106","many bioinformatics programming tasks can be automated with chatgpt","stephen r. piccolo, paul denny, andrew luxton-reilly, samuel payne, perry g. ridge","other quantitative biology","computer programming is a fundamental tool for life scientists, allowing them to carry out many essential research tasks. however, despite a variety of educational efforts, learning to write code can be a challenging endeavor for both researchers and students in life science disciplines. recent advances in artificial intelligence have made it possible to translate human-language prompts to functional code, raising questions about whether these technologies can aid (or replace) life scientists' efforts to write code. using 184 programming exercises from an introductory-bioinformatics course, we evaluated the extent to which one such model -- openai's chatgpt -- can successfully complete basic- to moderate-level programming tasks. on its first attempt, chatgpt solved 139 (75.5%) of the exercises. for the remaining exercises, we provided natural-language feedback to the model, prompting it to try different approaches. within 7 or fewer attempts, chatgpt solved 179 (97.3%) of the exercises. these findings have important implications for life-sciences research and education. for many programming tasks, researchers no longer need to write code from scratch. instead, machine-learning models may produce usable solutions. instructors may need to adapt their pedagogical approaches and assessment techniques to account for these new capabilities that are available to the general public.",2023-03-07
"107","holographic torus correlators of stress tensor in $ads_3/cft_2$","song he, yi li, yun-ze li, yunda zhang","high energy physics - theory","in the context of $\rm ads_3/cft_2$, we investigate holographic correlators of the stress tensor of a conformal field theory (cft) on a torus in this work. to calculate the correlators of the stress tensor, we employ the einstein-hilbert theory of gravity and perturbatively solve einstein's equation in the bulk. we offer an explicit prescription to develop a recurrence relation that makes it simple to compute higher point correlators. the correlators and the recurrence relation are found to be consistent with what is known in cfts. following the spirit of the proposed cutoff $\rm ads$/$t\bar{t}$ cft holography, we then expand our computation program to investigate holographic torus correlators at a finite cutoff in the $\rm ads_3$. a parallel recurrence relation associated with higher point correlators can be obtained.",2023-03-23
"108","probabilistic relations for modelling epistemic and aleatoric uncertainty: semantics and automated reasoning with theorem proving","kangfeng ye, jim woodcock, simon foster","logic in computer science","probabilistic programming combines general computer programming, statistical inference, and formal semantics to help systems make decisions when facing uncertainty. probabilistic programs are ubiquitous, including having a significant impact on machine intelligence. while many probabilistic algorithms have been used in practice in different domains, their automated verification based on formal semantics is still a relatively new research area. in the last two decades, it has attracted much interest. many challenges, however, remain. the work presented in this paper, probabilistic relations, takes a step towards our vision to tackle these challenges.
our work is based on hehner's predicative probabilistic programming, but there are several obstacles to the broader adoption of his work. our contributions here include (1) the formalisation of its syntax and semantics by introducing an iverson bracket notation to separate relations from arithmetic; (2) the formalisation of relations using unifying theories of programming (utp) and probabilities outside the brackets using summation over the topological space of the real numbers; (3) the constructive semantics for probabilistic loops using kleene's fixed-point theorem; (4) the enrichment of its semantics from distributions to subdistributions and superdistributions to deal with the constructive semantics; (5) the unique fixed-point theorem to simplify the reasoning about probabilistic loops; and (6) the mechanisation of our theory in isabelle/utp, an implementation of utp in isabelle/hol, for automated reasoning using theorem proving.
we demonstrate our work with six examples, including problems in robot localisation, classification in machine learning, and the termination of probabilistic loops.",2023-03-16
"109","wikicoder: learning to write knowledge-powered code","théo matricon, nathanaël fijalkow, gaëtan margueritte","machine learning","we tackle the problem of automatic generation of computer programs from a few pairs of input-output examples. the starting point of this work is the observation that in many applications a solution program must use external knowledge not present in the examples: we call such programs knowledge-powered since they can refer to information collected from a knowledge graph such as wikipedia. this paper makes a first step towards knowledge-powered program synthesis. we present wikicoder, a system building upon state of the art machine-learned program synthesizers and integrating knowledge graphs. we evaluate it to show its wide applicability over different domains and discuss its limitations. wikicoder solves tasks that no program synthesizers were able to solve before thanks to the use of knowledge graphs, while integrating with recent developments in the field to operate at scale.",2023-03-15
"110","gene expression programming for quantum computing","gonzalo alvarez, ryan bennink, stephan irle, jacek jakowski","quantum physics","we introduce quantumgep, a scientific computer program that uses gene expression programming (gep) to find a quantum circuit that either (i) maps a given set of input states to a given set of output states, or (ii) transforms a fixed initial state to minimize a given physical quantity of the output state. quantumgep is a driver program that uses evendim, a generic computational engine for gep, both of which are free and open source. we apply quantumgep as a powerful solver for maxcut in graphs, and for condensed matter quantum many-body hamiltonians.",2023-03-14
